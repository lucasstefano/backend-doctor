
import 'dotenv/config';

import express from 'express';
import http from 'http';
import { Server } from 'socket.io';
import speech from '@google-cloud/speech';
import multer from 'multer';
import cors from 'cors';
import { Storage } from '@google-cloud/storage';
import { v4 as uuidv4 } from 'uuid';

// ‚ú® Vertex AI
import { VertexAI } from '@google-cloud/vertexai';

// --- Fun√ß√µes de Log Padronizadas ---
const log = (prefix, message, ...args) =>
  console.log(`[${new Date().toISOString()}] [${prefix}]`, message, ...args);

// --- Configura√ß√µes iniciais ---
const app = express();
const server = http.createServer(app);
const io = new Server(server, { cors: { origin: '*' } });
const upload = multer();

app.use(cors());
app.use(express.json());

// --- Middleware de Logging de Requisi√ß√µes ---
app.use((req, res, next) => {
  log('HTTP', `Requisi√ß√£o recebida: ${req.method} ${req.url}`);
  next();
});

// --- Clientes ---
const speechClient = new speech.SpeechClient();
const storage = new Storage();
const vertex_ai = new VertexAI({
  project: process.env.GCLOUD_PROJECT,
  location: process.env.GCLOUD_LOCATION,
});

const model = 'gemini-2.0-flash-001'; // Modelo atualizado
const generativeModel = vertex_ai.getGenerativeModel({
  model,
  generationConfig: {
    maxOutputTokens: 8192,
    temperature: 0.2,
  },
});

/**
 * Fun√ß√£o auxiliar para chamar o Vertex AI e centralizar o logging.
 */
const callVertexAI = async (endpointName, prompt, generationConfig = {}) => {
  log('VertexAI', `Iniciando chamada para o endpoint: ${endpointName}`);
  log(
    'VertexAI',
    `Prompt enviado:\n---IN√çCIO DO PROMPT---\n${prompt}\n---FIM DO PROMPT---`
  );

  const request = {
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: { ...generativeModel.generationConfig, ...generationConfig },
  };

  const result = await generativeModel.generateContent(request);
  const generatedText = result.response.candidates[0].content.parts[0].text;

  log('VertexAI', `Resposta recebida do endpoint ${endpointName}`);
  return generatedText.trim();
};

// ===================================
// --- WebSocket STT com Automa√ß√£o ---
// ===================================
io.on('connection', (socket) => {
  log('WebSocket', `Cliente conectado: ${socket.id}`);

  let recognizeStream = null;
  let recognitionConfig = null;
  let silenceTimer = null;
  let streamRestartTimer = null;
  const silenceTimeoutDuration = 10000; // 10 segundos
  const maxStreamDuration = 290 * 1000; // ~4.8 minutos

  const stopRecognizeStream = () => {
    if (recognizeStream) {
      recognizeStream.end();
      recognizeStream = null;
      log('WebSocket', `Stream de reconhecimento encerrado para: ${socket.id}`);
    }
    clearTimeout(streamRestartTimer);
    clearTimeout(silenceTimer);
  };

  const startRecognizeStream = () => {
    if (recognizeStream || !recognitionConfig) {
      log(
        'WebSocket',
        `Tentativa de iniciar stream falhou (j√° iniciado ou sem config) para: ${socket.id}`
      );
      return;
    }
    log(
      'WebSocket',
      `Iniciando/Reiniciando stream de reconhecimento para: ${socket.id}`
    );

    const request = { config: recognitionConfig, interimResults: true };

    recognizeStream = speechClient
      .streamingRecognize(request)
      .on('error', (err) => {
        log('SpeechAPI-ERROR', `Erro no streaming para ${socket.id}:`, err.message);
        socket.emit('error', 'Erro no reconhecimento de fala.');
        stopRecognizeStream();
      })
      .on('data', (data) => {
        const result = data.results[0];
        if (result && result.alternatives[0]) {
          socket.emit('transcript-data', {
            text: result.alternatives[0].transcript,
            isFinal: result.isFinal,
            timestamp: new Date().toLocaleTimeString('pt-BR', {
              hour: '2-digit',
              minute: '2-digit',
            }),
            speakerTag:
              result.alternatives[0].words?.[
                result.alternatives[0].words.length - 1
              ]?.speakerTag,
          });
        }
      });

    // rein√≠cio autom√°tico do stream depois de 4.8 minutos
    streamRestartTimer = setTimeout(() => {
      log(
        'WebSocket',
        `Stream atingiu a dura√ß√£o m√°xima de ${
          maxStreamDuration / 1000
        }s. Reiniciando para ${socket.id}...`
      );
      stopRecognizeStream();
      startRecognizeStream();
    }, maxStreamDuration);
  };

  const resetSilenceTimer = () => {
    clearTimeout(silenceTimer);
    silenceTimer = setTimeout(() => {
      log(
        'WebSocket',
        `Sil√™ncio detectado para ${socket.id}. Reiniciando recognizeStream no servidor.`
      );

      // üü¢ reinicia internamente sem pedir nada ao cliente
      stopRecognizeStream();
      startRecognizeStream();
    }, silenceTimeoutDuration);
  };

  socket.on('start-recording', (config) => {
    log('WebSocket', `Evento 'start-recording' recebido de ${socket.id}`, config);
    recognitionConfig = {
      encoding: 'WEBM_OPUS',
      sampleRateHertz: config.sampleRateHertz || 48000,
      languageCode: config.lang || 'pt-BR',
      enableAutomaticPunctuation: true,
      diarizationConfig: {
        enableSpeakerDiarization: true,
        minSpeakerCount: 2,
        maxSpeakerCount: 6,
      },
      model: 'telephony',
      useEnhanced: true,
    };
    stopRecognizeStream();
    startRecognizeStream();
    resetSilenceTimer();
  });

  socket.on('audio-data', (data) => {
    if (recognizeStream && data) {
      recognizeStream.write(data);
      resetSilenceTimer();
    } else if (!recognizeStream) {
      log(
        'WebSocket',
        `Recebido 'audio-data' de ${socket.id}, mas o stream n√£o est√° pronto. Ignorando chunk.`
      );
    }
  });

  socket.on('force-flush-partial', (partial) => {
    log('WebSocket', `Evento 'force-flush-partial' recebido de ${socket.id}`);
    socket.emit('transcript-data', { ...partial, isFinal: true });
  });

  socket.on('stop-recording', () => {
    log('WebSocket', `Evento 'stop-recording' recebido de ${socket.id}`);
    stopRecognizeStream();
  });

  socket.on('disconnect', () => {
    log('WebSocket', `Cliente desconectado: ${socket.id}`);
    stopRecognizeStream();
  });
});

// ======================
// --- Batch STT ---
// ======================
app.post('/batch-transcribe', upload.single('file'), async (req, res) => {
  const endpointName = '/batch-transcribe';
  log('API', `Iniciando ${endpointName}`);
  try {
    if (!req.file) {
      log('API-ERROR', `${endpointName} - Nenhum arquivo enviado.`);
      return res.status(400).json({ error: 'Nenhum arquivo enviado.' });
    }

    const audioBuffer = req.file.buffer;
    const bucketName = process.env.GCLOUD_BUCKET_NAME;
    const recordingId = uuidv4();
    const filename = `audio-${recordingId}.opus`;
    const gcsUri = `gs://${bucketName}/${filename}`;

    log('GCS', `Fazendo upload de ${filename} para o bucket ${bucketName}`);
    await storage.bucket(bucketName).file(filename).save(audioBuffer, {
      metadata: { contentType: req.file.mimetype },
    });
    log('GCS', `Upload conclu√≠do: ${gcsUri}`);

    log('SpeechAPI', `Iniciando 'longRunningRecognize' para ${gcsUri}`);
    const [operation] = await speechClient.longRunningRecognize({
      audio: { uri: gcsUri },
      config: {
        encoding: 'WEBM_OPUS',
        sampleRateHertz: 48000,
        languageCode: 'pt-BR',
        enableAutomaticPunctuation: true,
        diarizationConfig: {
          enableSpeakerDiarization: true,
          minSpeakerCount: 2,
          maxSpeakerCount: 6,
        },
      },
    });

    const [response] = await operation.promise();
    log('SpeechAPI', `'longRunningRecognize' conclu√≠do para ${gcsUri}`);

    const structuredTranscript = [];
    let currentSegment = null;
    let lastSpeakerTag = null;

    response.results.forEach(result => {
      const alternative = result.alternatives[0];
      const transcriptText = alternative?.transcript?.trim();
      const words = alternative?.words;

      if (!transcriptText || !words || words.length === 0) return;

      const firstWord = words[0];
      const speakerTag = firstWord.speakerTag;

      if (speakerTag !== lastSpeakerTag) {
        currentSegment = {
          text: transcriptText,
          isFinal: true,
          speakerTag: speakerTag,
          timestamp: firstWord.startTime.seconds ? new Date(firstWord.startTime.seconds * 1000).toLocaleTimeString('pt-BR', { hour: '2-digit', minute: '2-digit' }) : '00:00',
        };
        structuredTranscript.push(currentSegment);
        lastSpeakerTag = speakerTag;
      } else if (currentSegment) {
        currentSegment.text += ' ' + transcriptText;
      }
    });

    log('API', `Transcri√ß√£o em lote estruturada com ${structuredTranscript.length} segmentos.`);
    res.json({ recordingId, audioUri: gcsUri, batchTranscript: structuredTranscript });

  } catch (err) {
    log('API-ERROR', `Erro em ${endpointName}:`, err);
    res.status(500).json({ error: 'Falha na transcri√ß√£o em lote.' });
  }
});


// ==========================
// --- Endpoints Vertex AI ---
// ==========================

// --- GERA√á√ÉO DE T√çTULO ---
app.post('/api/generate-title', async (req, res) => {
  const endpointName = '/api/generate-title';
  try {
    const { context } = req.body;
    if (!context || typeof context !== 'string' || context.trim() === '') {
      return res.status(400).json({ error: 'O campo "context" √© obrigat√≥rio.' });
    }

    const prompt = `
      Voc√™ √© um assistente especializado em criar t√≠tulos curtos e objetivos para consultas m√©dicas.
      Baseado no contexto abaixo, gere um t√≠tulo conciso (m√°x. 10 palavras) que resuma o motivo principal da consulta.
      O t√≠tulo deve ser claro, direto e f√°cil de entender. N√£o use markdown (como **, #) na resposta.

      Contexto: "${context}"

      T√≠tulo Gerado:
    `;

    const generatedTitle = await callVertexAI(endpointName, prompt);
    res.status(200).json({ title: generatedTitle });

  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar o t√≠tulo.' });
  }
});


// --- MELHORAR ANAMNESE ---
app.post('/api/melhorar-anamnese', async (req, res) => {
    const endpointName = '/api/melhorar-anamnese';
    try {
        const { anamnese, prompt } = req.body;

        if (!anamnese || typeof anamnese !== 'string' || anamnese.trim() === '') {
            return res.status(400).json({ error: 'O campo "anamnese" √© obrigat√≥rio.' });
        }
        if (!prompt || typeof prompt !== 'string' || prompt.trim() === '') {
            return res.status(400).json({ error: 'O campo "prompt" (instru√ß√£o) √© obrigat√≥rio.' });
        }

        const structuredPrompt = `
            ### Persona
            Aja como um assistente m√©dico redator, especialista em criar documentos cl√≠nicos claros, objetivos e bem estruturados.

            ### Contexto
            O texto de uma anamnese m√©dica precisa ser refinado com base em uma instru√ß√£o espec√≠fica do m√©dico.

            ### Tarefa
            Reescreva o "Texto Original da Anamnese" abaixo, seguindo estritamente a "Instru√ß√£o do M√©dico".

            ### Requisitos
            - O formato da resposta DEVE ser um √∫nico bloco de texto usando tags HTML simples (<p>, <strong>, <ul>, <li>).
            - O tom deve ser formal, t√©cnico e objetivo.
            - Mantenha TODAS as informa√ß√µes cl√≠nicas originais. N√ÉO omita e N√ÉO invente dados.
            - Corrija erros gramaticais.

            ### Dados de Entrada
            **Instru√ß√£o do M√©dico:**
            """
            ${prompt}
            """

            **Texto Original da Anamnese:**
            """
            ${anamnese}
            """
        `;

        const enhancedAnamnese = await callVertexAI(endpointName, structuredPrompt);
        res.status(200).json({ enhancedAnamnese });

    } catch (error) {
        log('API-ERROR', `Erro em ${endpointName}:`, error);
        res.status(500).json({ error: 'Ocorreu um erro no servidor ao processar a solicita√ß√£o.' });
    }
});

// --- ROTA DE TRANSCRI√á√ÉO IA (CORRIGE E IDENTIFICA FALANTES) ---
// app.post('/api/generate-ia-transcription', ...) - Substitua sua rota por esta

app.post('/api/generate-ia-transcription', async (req, res) => {
  try {
    const { transcription } = req.body;

    if (!transcription || typeof transcription !== 'string' || transcription.trim() === '') {
      return res.status(400).json({
        error: 'O campo "transcription" com o array de transcri√ß√µes √© obrigat√≥rio.'
      });
    }

    let allTranscripts;
    try {
      allTranscripts = JSON.parse(transcription);
      if (!Array.isArray(allTranscripts)) throw new Error("Formato inv√°lido.");
    } catch (e) {
      return res.status(400).json({ error: 'O campo "transcription" deve ser um JSON array v√°lido.' });
    }

    const context = allTranscripts.slice(0, -1);
    const newTranscriptToProcess = allTranscripts.slice(-1);

    const contextString = context.length > 0
      ? `
Contexto da Conversa (di√°logo anterior):
${JSON.stringify(context.map(t => ({ speaker: t.speaker, text: t.text })), null, 2)}
`
      : "Esta √© a primeira fala da conversa.";

    // üß† PASSO 1: Modificar o prompt para incluir a tarefa da TIMELINE
const prompt = `
Voc√™ √© um assistente de IA especialista em processar transcri√ß√µes de consultas m√©dicas.
Suas tarefas s√£o:
1. Analisar a "Nova Transcri√ß√£o", usando o "Contexto da Conversa" para manter a consist√™ncia na identifica√ß√£o de "M√©dico" e "Paciente".
2. Criar e atualizar uma "timeline" (uma lista cronol√≥gica) dos principais assuntos discutidos em TODA a conversa (contexto + nova transcri√ß√£o).
3. Formatar a sa√≠da como um objeto JSON contendo a transcri√ß√£o processada e a timeline de assuntos.

Instru√ß√µes Detalhadas:
1. Processamento da Transcri√ß√£o:
   - Corrija erros gramaticais na "Nova Transcri√ß√£o".
   - Mantenha a consist√™ncia dos pap√©is ("M√©dico", "Paciente").
   - A transcri√ß√£o processada deve ser um array de objetos, cada objeto representando uma fala √∫nica, com os campos:
     - "speakerTag"
     - "speaker"
     - "text"
     - "timestamp"
     - "isFinal"

2. Gera√ß√£o da Timeline de Assuntos:
   - Analise o di√°logo completo (contexto + nova transcri√ß√£o).
   - Identifique os t√≥picos principais (ex: "Apresenta√ß√£o de sintomas", "Hist√≥rico do paciente", "Discuss√£o sobre dor de cabe√ßa", "Diagn√≥stico inicial", "Prescri√ß√£o de medica√ß√£o").
   - A timeline deve ser um array de strings.
   - A cada nova chamada, voc√™ deve retornar a timeline completa e atualizada, adicionando novos t√≥picos conforme eles surgem.

3. Formato de Sa√≠da OBRIGAT√ìRIO:
   - Sua resposta DEVE SER um √∫nico objeto JSON, sem nenhum texto ou markdown em volta.
   - O objeto deve ter duas chaves: "processedTranscript" (um array de objetos, cada um representando uma fala) e "timeline" (um array de strings).

---
${contextString}
---

Nova Transcri√ß√£o para processar:
${JSON.stringify(newTranscriptToProcess, null, 2)}
`;


    const request = {
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig: {
        maxOutputTokens: 4048,
        temperature: 0.3, // Um pouco mais de temperatura para ajudar na criatividade da timeline
      },
    };

    const result = await generativeModel.generateContent(request);
    let generatedText = result.response?.candidates?.[0]?.content?.parts?.[0]?.text;

    if (!generatedText) {
      throw new Error('Resposta vazia do modelo generativo');
    }
    
    generatedText = generatedText.trim().replace(/^```json\s*|```$/g, "").trim();
    
    let parsedJson;
    try {
      // üß† PASSO 2: Fazer o parse da estrutura JSON completa { processedTranscript, timeline }
      parsedJson = JSON.parse(generatedText);
      // Valida√ß√£o b√°sica da estrutura
      if (!parsedJson.processedTranscript || !Array.isArray(parsedJson.timeline)) {
        throw new Error("A resposta da IA n√£o cont√©m os campos 'processedTranscript' e 'timeline'.");
      }
    } catch (err) {
      console.error("Erro ao fazer parse do JSON da IA:", err, generatedText);
      return res.status(500).json({ error: "Falha ao processar resposta da IA. Formato JSON inv√°lido." });
    }

    // üß† PASSO 3: Retornar o objeto completo no campo 'data'
    res.status(200).json({ data: parsedJson });

  } catch (error) {
    console.error('Erro ao gerar transcri√ß√£o via Vertex AI:', error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao processar a transcri√ß√£o.' });
  }
});

// --- GERA√á√ÉO DE RESUMO ---
app.post('/api/generate-summary', async (req, res) => {
  const endpointName = '/api/generate-summary';
  try {
    const { transcription } = req.body;
    if (!transcription || !Array.isArray(transcription) || transcription.length === 0) {
      return res.status(400).json({ error: 'O campo "transcription" √© obrigat√≥rio e deve ser um array.' });
    }

    const formattedTranscription = transcription.map(item => `${item.speakerTag || 'Pessoa'}: ${item.text}`).join('\n');

    const prompt = `
      Voc√™ √© um assistente de IA focado em transcri√ß√µes m√©dicas. Sua tarefa √© gerar dois resultados claros, sem usar markdown ou introdu√ß√µes.

      1. **Resumo da Transcri√ß√£o**: Crie um resumo objetivo da consulta.
      2. **Avalia√ß√£o da Transcri√ß√£o**:
         - Comente se a transcri√ß√£o cont√©m informa√ß√µes suficientes e coerentes.
         - Aponte lacunas ou inconsist√™ncias.
         - Avalie se faz sentido, no contexto m√©dico, usar IA para gerar resumos desta transcri√ß√£o.

      A transcri√ß√£o √© a seguinte:
      "${formattedTranscription}"
    `;

    const generatedSummary = await callVertexAI(endpointName, prompt);
    res.status(200).json({ summary: generatedSummary });

  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar o resumo.' });
  }
});

// --- GERA√á√ÉO DE ANAMNESE ---
app.post('/api/generate-anamnese', async (req, res) => {
  const endpointName = '/api/generate-anamnese';
  try {
    const { transcription, prompt, documentoSelecionado } = req.body;

    if (!transcription || !Array.isArray(transcription) || transcription.length === 0 || !documentoSelecionado) {
      return res.status(400).json({ error: 'Campos obrigat√≥rios: transcription, documentoSelecionado.' });
    }

    const formattedTranscript = transcription.map(line => `${line.speakerTag}: ${line.text}`).join('\n');

    const fullPrompt = `
      Voc√™ √© um assistente m√©dico virtual que sumariza conversas cl√≠nicas em anamneses estruturadas.
      Sua tarefa √© analisar a transcri√ß√£o de uma consulta e gerar uma anamnese completa.

      Instru√ß√µes Adicionais:
      ${prompt ? ` - Contexto do Paciente: "${prompt}"` : ''}
      
      O Documento deve conter as seguintes se√ß√µes obrigat√≥rias:
      ${documentoSelecionado}

      Formate o resultado em um √∫nico bloco de texto usando HTML (par√°grafos, negrito, listas).

      Transcri√ß√£o da consulta:
      "${formattedTranscript}"

      Anamnese Gerada (formato HTML):
    `;

    const generatedAnamnese = await callVertexAI(endpointName, fullPrompt);
    res.status(200).json({ anamnese: generatedAnamnese });

  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar a anamnese.' });
  }
});


// --- OBTEN√á√ÉO DE URL DE √ÅUDIO ---
app.get('/audio-url/:recordingId', async (req, res) => {
  const endpointName = '/audio-url/:recordingId';
  try {
    const bucketName = process.env.GCLOUD_BUCKET_NAME;
    const { recordingId } = req.params;
    const filename = `audio-${recordingId}.opus`;

    log('GCS', `Buscando URL assinada para ${filename}`);
    const file = storage.bucket(bucketName).file(filename);

    const [exists] = await file.exists();
    if (!exists) {
      log('GCS-ERROR', `Arquivo n√£o encontrado: ${filename}`);
      return res.status(404).json({ error: 'Arquivo de √°udio n√£o encontrado.' });
    }

    const [signedUrl] = await file.getSignedUrl({
      action: 'read',
      expires: Date.now() + 15 * 60 * 1000, // 15 minutos
    });

    log('GCS', `URL assinada gerada com sucesso.`);
    res.json({ audioUrl: signedUrl });

  } catch (err) {
    log('API-ERROR', `Erro em ${endpointName}:`, err);
    res.status(500).json({ error: 'Falha ao gerar URL de √°udio.' });
  }
});
app.delete('/audio/:recordingId', async (req, res) => {
  const endpointName = '/audio/:recordingId';
  try {
    // 1. O par√¢metro `recordingId` j√° vem decodificado pelo Express.
    // Se o frontend enviou um nome de arquivo codificado, aqui ele j√° estar√° no formato original.
    const { recordingId } = req.params;

    if (!recordingId) {
      return res.status(400).json({ error: 'ID de grava√ß√£o inv√°lido.' });
    }

    // 2. Obter o nome do bucket a partir das vari√°veis de ambiente.
    // Certifique-se de que a vari√°vel GCLOUD_BUCKET_NAME est√° definida no seu ambiente (.env ou no servidor).
    const bucketName = process.env.GCLOUD_BUCKET_NAME;
    if (!bucketName) {
        console.error('A vari√°vel de ambiente GCLOUD_BUCKET_NAME n√£o est√° definida.');
        return res.status(500).json({ error: 'Configura√ß√£o do servidor incompleta.'});
    }

    // 3. Obter a refer√™ncia do arquivo no Google Cloud Storage.
    const file = storage.bucket(bucketName).file(recordingId);

    // 4. Verificar se o arquivo realmente existe antes de tentar deletar.
    const [exists] = await file.exists();
    if (!exists) {
      console.log(`Arquivo n√£o encontrado no bucket '${bucketName}': ${recordingId}`);
      return res.status(404).json({ error: 'Arquivo de √°udio n√£o encontrado.' });
    }

    // 5. Deletar o arquivo.
    await file.delete();
    console.log(`Arquivo ${recordingId} removido do bucket ${bucketName} com sucesso.`);
    res.status(200).json({ message: 'Arquivo de √°udio removido com sucesso.' });

  } catch (err) {
    // 6. Tratamento de erros (ex: problemas de permiss√£o, falha na API do Google).
    console.error(`Erro em ${endpointName}:`, err);
    res.status(500).json({ error: 'Falha interna ao remover arquivo de √°udio.' });
  }
});

app.post("/api/upload-documento", upload.single("file"), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: "Nenhum arquivo enviado." });
    }

    const bucketName = process.env.GCLOUD_BUCKET_DOC; // certifique-se de ter essa vari√°vel no .env
    const bucket = storage.bucket(bucketName); // define o bucket
    const gcsFileName = `${Date.now()}_${req.file.originalname}`;
    const file = bucket.file(gcsFileName);

    // Upload do arquivo
    await file.save(req.file.buffer, {
      contentType: req.file.mimetype,
      resumable: false,
    });

    const publicUrl = `https://storage.googleapis.com/${bucketName}/${gcsFileName}`;
    res.json({ url: publicUrl });
  } catch (error) {
    console.error("Erro ao enviar arquivo:", error);
    res.status(500).json({ error: "Erro ao enviar arquivo." });
  }
});

app.post("/api/process-and-summarize-documents", upload.array("documentos", 5), async (req, res) => {
  const endpointName = "/api/process-and-summarize-documents";
  log('API', `Iniciando ${endpointName}`);

  if (!req.files || req.files.length === 0) {
    return res.status(400).json({ error: "Nenhum documento enviado." });
  }

  try {
    const summaryPromises = req.files.map(async (file) => {
      log('VertexAI', `Processando arquivo: ${file.originalname} (${file.mimetype})`);

      const fileBuffer = file.buffer;
      const mimeType = file.mimetype;

      // Inicializa o array de partes
      const promptParts = [];
      let promptText = "";

      const supportedImage = mimeType.startsWith('image/');
      const supportedDoc = ['application/pdf', 'text/plain', 'text/markdown'].includes(mimeType);

      if (supportedImage) {
        promptText = "Voc√™ √© um assistente m√©dico. Descreva esta imagem de forma objetiva, focando em detalhes que possam ser clinicamente relevantes. Se for um exame, descreva os achados. Se for um documento, extraia o texto e resuma-o.";
      } else if (supportedDoc) {
        promptText = "Voc√™ √© um assistente m√©dico. Resuma o conte√∫do deste documento, extraindo as informa√ß√µes mais importantes como diagn√≥sticos, tratamentos, resultados de exames e hist√≥rico do paciente.";
      } else {
        log('VertexAI', `Tipo de arquivo n√£o suportado para resumo: ${mimeType}`);
        return {
          fileName: file.originalname,
          summary: [`Resumo n√£o gerado para '${file.originalname}' - tipo de arquivo n√£o suportado`],
        };
      }

      // Adiciona as partes ao array promptParts
      promptParts.push({ text: promptText });
      promptParts.push({
        inlineData: {
          mimeType: mimeType,
          data: fileBuffer.toString('base64'),
        },
      });

      // Estrutura final da requisi√ß√£o
      const request = {
        contents: [{ role: 'user', parts: promptParts }],
        generationConfig: {
          maxOutputTokens: 2048,
          temperature: 0.3,
        },
      };

      // Log para debug
      console.log("Objeto da requisi√ß√£o enviado para a Vertex AI:", JSON.stringify(request, null, 2));

      const result = await generativeModel.generateContent(request);
      const summaryText = result.response?.candidates?.[0]?.content?.parts?.[0]?.text;

      if (summaryText) {
        return {
          fileName: file.originalname,
          summary: summaryText.trim(),
        };
      }

      return {
        fileName: file.originalname,
        summary: [`N√£o foi poss√≠vel gerar um resumo para '${file.originalname}'`],
      };
    });

    const summaries = await Promise.all(summaryPromises);

    log('API', `Resumos gerados com sucesso para ${endpointName}`);
    res.status(200).json({ summaries });
  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({
      error: "Ocorreu um erro no servidor ao processar os documentos.",
      details: error.message,
      stack: process.env.NODE_ENV === 'development' ? error.stack : undefined,
    });
  }
});

app.post("/api/chat", async (req, res) => {
  try {
    const {comando, history } = req.body;
    if (!history || !Array.isArray(history) || history.length === 0) {
      return res.status(400).json({ error: 'O campo "history" √© obrigat√≥rio.' });
    }

    const systemPrompt = `
      Voc√™ √© uma IA m√©dica, assistente de consultas. Sua sa√≠da deve ser EXCLUSIVAMENTE um JSON v√°lido.
      Nunca adicione explica√ß√µes, coment√°rios ou texto fora do JSON.
      Voc√™ deve analisar o hist√≥rico da conversa e o √∫ltimo comando do usu√°rio para determinar a resposta.

      Se o √∫ltimo comando for uma transcri√ß√£o de √°udio:
      - Gere um resumo cl√≠nico curto da transcri√ß√£o.
      - Crie um t√≠tulo conciso (at√© 10 palavras) para a consulta.
      - A sa√≠da deve ser um JSON com a estrutura:
        {
          "mensagem": "resumo cl√≠nico aqui",
          "titulo": "t√≠tulo da consulta aqui",
          "mode": "BIGTIME"
        }

      Se o √∫ltimo comando for uma solicita√ß√£o de anamnese, ou documento, como "Gera uma Anamnese" ou "Gere Documento":
      - Analise toda a conversa anterior.
      - Gere uma anamnese/documento completa sempre em formato HTML (usando par√°grafos, negrito, listas).
      - A sa√≠da deve ser um JSON com a estrutura:
        {
          "html": "anamnese/documento completa em HTML aqui",
           "titulo": "t√≠tulo para o documento aqui",
          "mode": "HTML"
        }

      Para qualquer outro comando ou pergunta do usu√°rio:
      - Responda de forma normal e √∫til para a conversa.
      - A sa√≠da deve ser um JSON com a estrutura:
        {
          "mensagem": "sua resposta normal aqui, pense bem antes de responder, analise o contexto geral",
          "mode": "CHATIME"
        }
    `;

    const formattedHistory = history.map(msg => ({
      role: msg.from === "user" ? "user" : "model",
      parts: [{ text: `${comando} - ${msg.text}` }]
    }));

    const previousMessages = formattedHistory.slice(0, -1);
    const lastUserMessage = formattedHistory[formattedHistory.length - 1].parts[0].text;

    const model = "gemini-2.5-pro";
    const generativeModel = vertex_ai.getGenerativeModel({
      model,
      generationConfig: { maxOutputTokens: 4048, temperature: 0.2 }
    });

    const chat = generativeModel.startChat({
      systemInstruction: { parts: [{ text: systemPrompt }] },
      history: previousMessages
    });

    const result = await chat.sendMessage(lastUserMessage);
    const responseText = result.response.candidates[0].content.parts[0].text;

    try {
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (jsonMatch && jsonMatch[0]) {
        const responseObject = JSON.parse(jsonMatch[0]);
        // Envia a resposta JSON diretamente, sem precisar de ifs
        res.json(responseObject);
      } else {
        console.warn("A resposta da IA n√£o continha um JSON v√°lido.");
        // Resposta de fallback caso a IA falhe
        res.status(500).json({ mensagem: "Erro: formato de resposta da IA inv√°lido." });
      }
    } catch (error) {
      console.error("Erro ao fazer parse do JSON da IA:", error);
      res.status(500).json({ mensagem: "Erro interno no servidor." });
    }

  } catch (error) {
    console.error(error);
    res.status(500).json({ error: "Erro ao processar a requisi√ß√£o de chat." });
  }
});

// --- Iniciar Servidor ---
const PORT = process.env.PORT || 3001;
server.listen(PORT, () => {
  log('Server', `üöÄ Servidor rodando na porta ${PORT}`);
});