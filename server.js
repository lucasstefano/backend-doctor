// index.js
import 'dotenv/config';

import express from 'express';
import http from 'http';
import { Server } from 'socket.io';
import speech from '@google-cloud/speech';
import multer from 'multer';
import cors from 'cors';
import { Storage } from '@google-cloud/storage';
import { v4 as uuidv4 } from 'uuid';

// Vertex AI
import { VertexAI } from '@google-cloud/vertexai';

// --- FunÃ§Ãµes de Log Padronizadas ---
const log = (prefix, message, ...args) =>
  console.log(`[${new Date().toISOString()}] [${prefix}]`, message, ...args);

// --- ConfiguraÃ§Ãµes iniciais ---
const app = express();
const server = http.createServer(app);

// Socket.io configurado para Cloud Run
const io = new Server(server, { 
  cors: { 
    origin: "*",
    methods: ["GET", "POST"]
  },
  transports: ['websocket', 'polling'],
  pingTimeout: 60000,
  pingInterval: 25000
});

const upload = multer();

// CORS configurado para Cloud Run
app.use(cors({
  origin: true,
  credentials: true,
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS']
}));

app.use(express.json());

// Health check endpoint para Cloud Run
app.get('/health', (req, res) => {
  res.status(200).json({ 
    status: 'ok', 
    socket: 'enabled',
    timestamp: new Date().toISOString()
  });
});

// WebSocket test endpoint
app.get('/ws-test', (req, res) => {
  res.json({ 
    status: 'ok', 
    socketEnabled: true,
    timestamp: new Date().toISOString()
  });
});

// Handle preflight
app.options('*', cors());

// --- Middleware de Logging de RequisiÃ§Ãµes ---
app.use((req, res, next) => {
  log('HTTP', `RequisiÃ§Ã£o recebida: ${req.method} ${req.url}`);
  next();
});

// --- Clientes ---
const speechClient = new speech.SpeechClient();
const storage = new Storage();
const vertex_ai = new VertexAI({
  project: process.env.GCLOUD_PROJECT,
  location: process.env.GCLOUD_LOCATION,
});

const model = 'gemini-2.0-flash-001';
const generativeModel = vertex_ai.getGenerativeModel({
  model,
  generationConfig: {
    maxOutputTokens: 8192,
    temperature: 0.2,
  },
});

/**
 * FunÃ§Ã£o auxiliar para chamar o Vertex AI e centralizar o logging.
 */
const callVertexAI = async (endpointName, prompt, generationConfig = {}) => {
  log('VertexAI', `Iniciando chamada para o endpoint: ${endpointName}`);
  log(
    'VertexAI',
    `Prompt enviado:\n---INÃCIO DO PROMPT---\n${prompt}\n---FIM DO PROMPT---`
  );

  const request = {
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: { ...generativeModel.generationConfig, ...generationConfig },
  };

  const result = await generativeModel.generateContent(request);
  const generatedText = result.response.candidates[0].content.parts[0].text;

  log('VertexAI', `Resposta recebida do endpoint ${endpointName}`);
  return generatedText.trim();
};

// ===================================
// --- WebSocket STT com AutomaÃ§Ã£o ---
// ===================================
io.on('connection', (socket) => {
  log('WebSocket', `Cliente conectado: ${socket.id} from ${socket.handshake.address}`);

  let recognizeStream = null;
  let recognitionConfig = null;
  let silenceTimer = null;
  let streamRestartTimer = null;
  const silenceTimeoutDuration = 10000; // 10 segundos
  const maxStreamDuration = 290 * 1000; // ~4.8 minutos

  const stopRecognizeStream = () => {
    if (recognizeStream) {
      recognizeStream.end();
      recognizeStream = null;
      log('WebSocket', `Stream de reconhecimento encerrado para: ${socket.id}`);
    }
    clearTimeout(streamRestartTimer);
    clearTimeout(silenceTimer);
  };

  const startRecognizeStream = () => {
    if (recognizeStream || !recognitionConfig) {
      log(
        'WebSocket',
        `Tentativa de iniciar stream falhou (jÃ¡ iniciado ou sem config) para: ${socket.id}`
      );
      return;
    }
    log(
      'WebSocket',
      `Iniciando/Reiniciando stream de reconhecimento para: ${socket.id}`
    );

    const request = { config: recognitionConfig, interimResults: true };

    recognizeStream = speechClient
      .streamingRecognize(request)
      .on('error', (err) => {
        log('SpeechAPI-ERROR', `Erro no streaming para ${socket.id}:`, err.message);
        socket.emit('error', 'Erro no reconhecimento de fala.');
        stopRecognizeStream();
      })
      .on('data', (data) => {
        const result = data.results[0];
        if (result && result.alternatives[0]) {
          const transcriptData = {
            text: result.alternatives[0].transcript,
            isFinal: result.isFinal,
            timestamp: new Date().toLocaleTimeString('pt-BR', {
              hour: '2-digit',
              minute: '2-digit',
            }),
            speakerTag:
              result.alternatives[0].words?.[
                result.alternatives[0].words.length - 1
              ]?.speakerTag,
          };
          
          log('WebSocket', `Enviando transcript: ${transcriptData.text.substring(0, 50)}...`);
          socket.emit('transcript-data', transcriptData);
        }
      });

    // reinÃ­cio automÃ¡tico do stream depois de 4.8 minutos
    streamRestartTimer = setTimeout(() => {
      log(
        'WebSocket',
        `Stream atingiu a duraÃ§Ã£o mÃ¡xima de ${
          maxStreamDuration / 1000
        }s. Reiniciando para ${socket.id}...`
      );
      stopRecognizeStream();
      startRecognizeStream();
    }, maxStreamDuration);
  };

  const resetSilenceTimer = () => {
    clearTimeout(silenceTimer);
    silenceTimer = setTimeout(() => {
      log(
        'WebSocket',
        `SilÃªncio detectado para ${socket.id}. Reiniciando recognizeStream no servidor.`
      );

      // ğŸŸ¢ reinicia internamente sem pedir nada ao cliente
      stopRecognizeStream();
      startRecognizeStream();
    }, silenceTimeoutDuration);
  };

  socket.on('start-recording', (config) => {
    log('WebSocket', `Evento 'start-recording' recebido de ${socket.id}`, config);
    recognitionConfig = {
      encoding: 'WEBM_OPUS',
      sampleRateHertz: config.sampleRateHertz || 48000,
      languageCode: config.lang || 'pt-BR',
      alternativeLanguageCodes: ['en-US'], 
      enableAutomaticPunctuation: true,
      diarizationConfig: {
        enableSpeakerDiarization: true,
        minSpeakerCount: 2,
        maxSpeakerCount: 6,
      },
      model: 'telephony',
      useEnhanced: true,
    };
    stopRecognizeStream();
    startRecognizeStream();
    resetSilenceTimer();
  });

  socket.on('audio-data', (data) => {
    if (recognizeStream && data) {
      recognizeStream.write(data);
      resetSilenceTimer();
    } else if (!recognizeStream) {
      log(
        'WebSocket',
        `Recebido 'audio-data' de ${socket.id}, mas o stream nÃ£o estÃ¡ pronto. Ignorando chunk.`
      );
    }
  });

  socket.on('force-flush-partial', (partial) => {
    log('WebSocket', `Evento 'force-flush-partial' recebido de ${socket.id}`);
    socket.emit('transcript-data', { ...partial, isFinal: true });
  });

  socket.on('stop-recording', () => {
    log('WebSocket', `Evento 'stop-recording' recebido de ${socket.id}`);
    stopRecognizeStream();
  });

  socket.on('disconnect', (reason) => {
    log('WebSocket', `Cliente desconectado: ${socket.id} - Reason: ${reason}`);
    stopRecognizeStream();
  });
});

// ======================
// --- Batch STT ---
// ======================
app.post('/batch-transcribe', upload.single('file'), async (req, res) => {
  const endpointName = '/batch-transcribe';
  log('API', `Iniciando ${endpointName}`);
  try {
    if (!req.file) {
      log('API-ERROR', `${endpointName} - Nenhum arquivo enviado.`);
      return res.status(400).json({ error: 'Nenhum arquivo enviado.' });
    }

    const audioBuffer = req.file.buffer;
    const bucketName = process.env.GCLOUD_BUCKET_NAME;
    const recordingId = uuidv4();
    const filename = `audio-${recordingId}.opus`;
    const gcsUri = `gs://${bucketName}/${filename}`;

    log('GCS', `Fazendo upload de ${filename} para o bucket ${bucketName}`);
    await storage.bucket(bucketName).file(filename).save(audioBuffer, {
      metadata: { contentType: req.file.mimetype },
    });
    log('GCS', `Upload concluÃ­do: ${gcsUri}`);

    log('SpeechAPI', `Iniciando 'longRunningRecognize' para ${gcsUri}`);
    const [operation] = await speechClient.longRunningRecognize({
      audio: { uri: gcsUri },
      config: {
        encoding: 'WEBM_OPUS',
        sampleRateHertz: 48000,
        languageCode: 'pt-BR',
        alternativeLanguageCodes: ['en-US'],
        enableAutomaticPunctuation: true,
        diarizationConfig: {
          enableSpeakerDiarization: true,
          minSpeakerCount: 2,
          maxSpeakerCount: 6,
        },
      },
    });

    const [response] = await operation.promise();
    log('SpeechAPI', `'longRunningRecognize' concluÃ­do para ${gcsUri}`);

    const structuredTranscript = [];
    let currentSegment = null;
    let lastSpeakerTag = null;

    response.results.forEach(result => {
      const alternative = result.alternatives[0];
      const transcriptText = alternative?.transcript?.trim();
      const words = alternative?.words;

      if (!transcriptText || !words || words.length === 0) return;

      const firstWord = words[0];
      const speakerTag = firstWord.speakerTag;

      if (speakerTag !== lastSpeakerTag) {
        currentSegment = {
          text: transcriptText,
          isFinal: true,
          speakerTag: speakerTag,
          timestamp: firstWord.startTime.seconds ? new Date(firstWord.startTime.seconds * 1000).toLocaleTimeString('pt-BR', { hour: '2-digit', minute: '2-digit' }) : '00:00',
        };
        structuredTranscript.push(currentSegment);
        lastSpeakerTag = speakerTag;
      } else if (currentSegment) {
        currentSegment.text += ' ' + transcriptText;
      }
    });

    log('API', `TranscriÃ§Ã£o em lote estruturada com ${structuredTranscript.length} segmentos.`);
    res.json({ recordingId, audioUri: gcsUri, batchTranscript: structuredTranscript });

  } catch (err) {
    log('API-ERROR', `Erro em ${endpointName}:`, err);
    res.status(500).json({ error: 'Falha na transcriÃ§Ã£o em lote.' });
  }
});

// ==========================
// --- Endpoints Vertex AI ---
// ==========================

// --- GERAÃ‡ÃƒO DE TÃTULO ---
app.post('/api/generate-title', async (req, res) => {
  const endpointName = '/api/generate-title';
  try {
    const { context } = req.body;
    if (!context || typeof context !== 'string' || context.trim() === '') {
      return res.status(400).json({ error: 'O campo "context" Ã© obrigatÃ³rio.' });
    }

    const prompt = `
      VocÃª Ã© um assistente especializado em criar tÃ­tulos curtos e objetivos para consultas mÃ©dicas.
      Baseado no contexto abaixo, gere um tÃ­tulo conciso (mÃ¡x. 10 palavras) que resuma o motivo principal da consulta.
      O tÃ­tulo deve ser claro, direto e fÃ¡cil de entender. NÃ£o use markdown (como **, #) na resposta.

      Contexto: "${context}"

      TÃ­tulo Gerado:
    `;

    const generatedTitle = await callVertexAI(endpointName, prompt);
    res.status(200).json({ title: generatedTitle });

  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar o tÃ­tulo.' });
  }
});

// --- MELHORAR ANAMNESE ---
app.post('/api/melhorar-anamnese', async (req, res) => {
    const endpointName = '/api/melhorar-anamnese';
    try {
        const { anamnese, prompt } = req.body;

        if (!anamnese || typeof anamnese !== 'string' || anamnese.trim() === '') {
            return res.status(400).json({ error: 'O campo "anamnese" Ã© obrigatÃ³rio.' });
        }
        if (!prompt || typeof prompt !== 'string' || prompt.trim() === '') {
            return res.status(400).json({ error: 'O campo "prompt" (instruÃ§Ã£o) Ã© obrigatÃ³rio.' });
        }

        const structuredPrompt = `
            ### Persona
            Aja como um assistente mÃ©dico redator, especialista em criar documentos clÃ­nicos claros, objetivos e bem estruturados.

            ### Contexto
            O texto de uma anamnese mÃ©dica precisa ser refinado com base em uma instruÃ§Ã£o especÃ­fica do mÃ©dico.

            ### Tarefa
            Reescreva o "Texto Original da Anamnese" abaixo, seguindo estritamente a "InstruÃ§Ã£o do MÃ©dico".

            ### Requisitos
            - O formato da resposta DEVE ser um Ãºnico bloco de texto usando tags HTML simples (<p>, <strong>, <ul>, <li>).
            - O tom deve ser formal, tÃ©cnico e objetivo.
            - Mantenha TODAS as informaÃ§Ãµes clÃ­nicas originais. NÃƒO omita e NÃƒO invente dados.
            - Corrija erros gramaticais.

            ### Dados de Entrada
            **InstruÃ§Ã£o do MÃ©dico:**
            """
            ${prompt}
            """

            **Texto Original da Anamnese:**
            """
            ${anamnese}
            """
        `;

        const enhancedAnamnese = await callVertexAI(endpointName, structuredPrompt);
        res.status(200).json({ enhancedAnamnese });

    } catch (error) {
        log('API-ERROR', `Erro em ${endpointName}:`, error);
        res.status(500).json({ error: 'Ocorreu um erro no servidor ao processar a solicitaÃ§Ã£o.' });
    }
});

// --- ROTA DE TRANSCRIÃ‡ÃƒO IA ---
app.post('/api/generate-ia-transcription', async (req, res) => {
  try {
    const { transcription } = req.body;

    if (!transcription || typeof transcription !== 'string' || transcription.trim() === '') {
      return res.status(400).json({
        error: 'O campo "transcription" com o array de transcriÃ§Ãµes Ã© obrigatÃ³rio.'
      });
    }

    let allTranscripts;
    try {
      allTranscripts = JSON.parse(transcription);
      if (!Array.isArray(allTranscripts)) throw new Error("Formato invÃ¡lido.");
    } catch (e) {
      return res.status(400).json({ error: 'O campo "transcription" deve ser um JSON array vÃ¡lido.' });
    }

    const context = allTranscripts.slice(0, -1);
    const newTranscriptToProcess = allTranscripts.slice(-1);

    const contextString = context.length > 0
      ? `
Contexto da Conversa (diÃ¡logo anterior):
${JSON.stringify(context.map(t => ({ speaker: t.speaker, text: t.text })), null, 2)}
`
      : "Esta Ã© a primeira fala da conversa.";

    const prompt = `
VocÃª Ã© um assistente de IA especialista em processar transcriÃ§Ãµes de consultas mÃ©dicas.

âš™ï¸ InstruÃ§Ã£o de Idioma:
- Detecte automaticamente o idioma da "Nova TranscriÃ§Ã£o".
- Se o idioma predominante for **inglÃªs**, todas as respostas e o conteÃºdo do JSON devem ser **em inglÃªs** (inclusive nomes de campos e tÃ³picos da timeline).
- Caso contrÃ¡rio, use **portuguÃªs** como idioma padrÃ£o.
- NÃƒO traduza o conteÃºdo da transcriÃ§Ã£o; apenas mantenha o idioma original da conversa para todo o processamento.

Suas tarefas sÃ£o:
1. Analisar a "Nova TranscriÃ§Ã£o", usando o "Contexto da Conversa" para manter a consistÃªncia na identificaÃ§Ã£o de "MÃ©dico" e "Paciente".
2. Criar e atualizar uma "timeline" (uma lista cronolÃ³gica) dos principais assuntos discutidos em TODA a conversa (contexto + nova transcriÃ§Ã£o).
3. Formatar a saÃ­da como um objeto JSON contendo a transcriÃ§Ã£o processada e a timeline de assuntos.

InstruÃ§Ãµes Detalhadas:
1. Processamento da TranscriÃ§Ã£o:
   - Corrija erros gramaticais na "Nova TranscriÃ§Ã£o".
   - Mantenha a consistÃªncia dos papÃ©is ("MÃ©dico", "Paciente").
   - A transcriÃ§Ã£o processada deve ser um array de objetos, cada objeto representando uma fala Ãºnica, com os campos obrigatÃ³rios:
     - "speakerTag"
     - "speaker"
     - "text"
     - "timestamp"
     - "isFinal"
   - ATENÃ‡ÃƒO: O campo "isFinal" DEVE SEMPRE ser \`true\` em todos os itens do processedTranscript.
   - Caso o texto esteja vazio ("" ou apenas espaÃ§os), nÃ£o deve ser incluÃ­do no processedTranscript.

2. GeraÃ§Ã£o da Timeline de Assuntos:
   - Analise o diÃ¡logo completo (contexto + nova transcriÃ§Ã£o).
   - Identifique os tÃ³picos principais (ex: "ApresentaÃ§Ã£o de sintomas", "HistÃ³rico do paciente", "DiscussÃ£o sobre dor de cabeÃ§a", "DiagnÃ³stico inicial", "PrescriÃ§Ã£o de medicaÃ§Ã£o").
   - A timeline deve ser um array de strings.
   - A cada nova chamada, vocÃª deve retornar a timeline completa e atualizada, adicionando novos tÃ³picos conforme eles surgem.

3. Formato de SaÃ­da OBRIGATÃ“RIO:
   - Sua resposta DEVE SER um Ãºnico objeto JSON, sem nenhum texto ou markdown em volta.
   - O objeto deve ter duas chaves: "processedTranscript" (um array de objetos, cada um representando uma fala) e "timeline" (um array de strings).

---
${contextString}
---
Nova TranscriÃ§Ã£o para processar:
${JSON.stringify(newTranscriptToProcess, null, 2)}
`;

    const request = {
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig: {
        maxOutputTokens: 4048,
        temperature: 0.3,
      },
    };

    const result = await generativeModel.generateContent(request);
    let generatedText = result.response?.candidates?.[0]?.content?.parts?.[0]?.text;

    if (!generatedText) {
      throw new Error('Resposta vazia do modelo generativo');
    }
    
    generatedText = generatedText.trim().replace(/^```json\s*|```$/g, "").trim();
    
    let parsedJson;
    try {
      parsedJson = JSON.parse(generatedText);
      if (!parsedJson.processedTranscript || !Array.isArray(parsedJson.timeline)) {
        throw new Error("A resposta da IA nÃ£o contÃ©m os campos 'processedTranscript' e 'timeline'.");
      }
    } catch (err) {
      console.error("Erro ao fazer parse do JSON da IA:", err, generatedText);
      return res.status(500).json({ error: "Falha ao processar resposta da IA. Formato JSON invÃ¡lido." });
    }

    res.status(200).json({ data: parsedJson });

  } catch (error) {
    console.error('Erro ao gerar transcriÃ§Ã£o via Vertex AI:', error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao processar a transcriÃ§Ã£o.' });
  }
});

// --- GERAÃ‡ÃƒO DE RESUMO ---
app.post('/api/generate-summary', async (req, res) => {
  const endpointName = '/api/generate-summary';
  try {
    const { transcription } = req.body;
    if (!transcription || !Array.isArray(transcription) || transcription.length === 0) {
      return res.status(400).json({ error: 'O campo "transcription" Ã© obrigatÃ³rio e deve ser um array.' });
    }

    const formattedTranscription = transcription.map(item => `${item.speakerTag || 'Pessoa'}: ${item.text}`).join('\n');

    const prompt = `
      VocÃª Ã© um assistente de IA focado em transcriÃ§Ãµes mÃ©dicas. Sua tarefa Ã© gerar dois resultados claros, sem usar markdown ou introduÃ§Ãµes.

      1. **Resumo da TranscriÃ§Ã£o**: Crie um resumo objetivo da consulta.
      2. **AvaliaÃ§Ã£o da TranscriÃ§Ã£o**:
         - Comente se a transcriÃ§Ã£o contÃ©m informaÃ§Ãµes suficientes e coerentes.
         - Aponte lacunas ou inconsistÃªncias.
         - Avalie se faz sentido, no contexto mÃ©dico, usar IA para gerar resumos desta transcriÃ§Ã£o.

      A transcriÃ§Ã£o Ã© a seguinte:
      "${formattedTranscription}"
    `;

    const generatedSummary = await callVertexAI(endpointName, prompt);
    res.status(200).json({ summary: generatedSummary });

  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar o resumo.' });
  }
});

// --- GERAÃ‡ÃƒO DE ANAMNESE ---
app.post('/api/generate-anamnese', async (req, res) => {
  const endpointName = '/api/generate-anamnese';
  try {
    const { transcription, prompt, documentoSelecionado } = req.body;

    if (!transcription || !Array.isArray(transcription) || transcription.length === 0 || !documentoSelecionado) {
      return res.status(400).json({ error: 'Campos obrigatÃ³rios: transcription, documentoSelecionado.' });
    }

    const formattedTranscript = transcription.map(line => `${line.speakerTag}: ${line.text}`).join('\n');

    const fullPrompt = `
      VocÃª Ã© um assistente mÃ©dico virtual que sumariza conversas clÃ­nicas em anamneses estruturadas.
      Sua tarefa Ã© analisar a transcriÃ§Ã£o de uma consulta e gerar uma anamnese completa.

      InstruÃ§Ãµes Adicionais:
      ${prompt ? ` - Contexto do Paciente: "${prompt}"` : ''}
      
      O Documento deve conter as seguintes seÃ§Ãµes obrigatÃ³rias:
      ${documentoSelecionado}

      Formate o resultado em um Ãºnico bloco de texto usando HTML (parÃ¡grafos, negrito, listas).

      TranscriÃ§Ã£o da consulta:
      "${formattedTranscript}"

      Anamnese Gerada (formato HTML):
    `;

    const generatedAnamnese = await callVertexAI(endpointName, fullPrompt);
    res.status(200).json({ anamnese: generatedAnamnese });

  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar a anamnese.' });
  }
});

// --- OBTENÃ‡ÃƒO DE URL DE ÃUDIO ---
app.get('/audio-url/:recordingId', async (req, res) => {
  const endpointName = '/audio-url/:recordingId';
  try {
    const bucketName = process.env.GCLOUD_BUCKET_NAME;
    const { recordingId } = req.params;
    const filename = `audio-${recordingId}.opus`;

    log('GCS', `Buscando URL assinada para ${filename}`);
    const file = storage.bucket(bucketName).file(filename);

    const [exists] = await file.exists();
    if (!exists) {
      log('GCS-ERROR', `Arquivo nÃ£o encontrado: ${filename}`);
      return res.status(404).json({ error: 'Arquivo de Ã¡udio nÃ£o encontrado.' });
    }

    const [signedUrl] = await file.getSignedUrl({
      action: 'read',
      expires: Date.now() + 15 * 60 * 1000, // 15 minutos
    });

    log('GCS', `URL assinada gerada com sucesso.`);
    res.json({ audioUrl: signedUrl });

  } catch (err) {
    log('API-ERROR', `Erro em ${endpointName}:`, err);
    res.status(500).json({ error: 'Falha ao gerar URL de Ã¡udio.' });
  }
});

app.delete('/audio/:recordingId', async (req, res) => {
  const endpointName = '/audio/:recordingId';
  try {
    const { recordingId } = req.params;

    if (!recordingId) {
      return res.status(400).json({ error: 'ID de gravaÃ§Ã£o invÃ¡lido.' });
    }

    const bucketName = process.env.GCLOUD_BUCKET_NAME;
    if (!bucketName) {
        console.error('A variÃ¡vel de ambiente GCLOUD_BUCKET_NAME nÃ£o estÃ¡ definida.');
        return res.status(500).json({ error: 'ConfiguraÃ§Ã£o do servidor incompleta.'});
    }

    const file = storage.bucket(bucketName).file(recordingId);

    const [exists] = await file.exists();
    if (!exists) {
      console.log(`Arquivo nÃ£o encontrado no bucket '${bucketName}': ${recordingId}`);
      return res.status(404).json({ error: 'Arquivo de Ã¡udio nÃ£o encontrado.' });
    }

    await file.delete();
    console.log(`Arquivo ${recordingId} removido do bucket ${bucketName} com sucesso.`);
    res.status(200).json({ message: 'Arquivo de Ã¡udio removido com sucesso.' });

  } catch (err) {
    console.error(`Erro em ${endpointName}:`, err);
    res.status(500).json({ error: 'Falha interna ao remover arquivo de Ã¡udio.' });
  }
});

app.post("/api/upload-documento", upload.single("file"), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: "Nenhum arquivo enviado." });
    }

    const bucketName = process.env.GCLOUD_BUCKET_DOC;
    const bucket = storage.bucket(bucketName);
    const gcsFileName = `${Date.now()}_${req.file.originalname}`;
    const file = bucket.file(gcsFileName);

    await file.save(req.file.buffer, {
      contentType: req.file.mimetype,
      resumable: false,
    });

    const publicUrl = `https://storage.googleapis.com/${bucketName}/${gcsFileName}`;
    res.json({ url: publicUrl });
  } catch (error) {
    console.error("Erro ao enviar arquivo:", error);
    res.status(500).json({ error: "Erro ao enviar arquivo." });
  }
});

app.post("/api/process-and-summarize-documents", upload.array("documentos", 5), async (req, res) => {
  const endpointName = "/api/process-and-summarize-documents";
  log('API', `Iniciando ${endpointName}`);

  if (!req.files || req.files.length === 0) {
    return res.status(400).json({ error: "Nenhum documento enviado." });
  }

  try {
    const summaryPromises = req.files.map(async (file) => {
      log('VertexAI', `Processando arquivo: ${file.originalname} (${file.mimetype})`);

      const fileBuffer = file.buffer;
      const mimeType = file.mimetype;

      const promptParts = [];
      let promptText = "";

      const supportedImage = mimeType.startsWith('image/');
      const supportedDoc = ['application/pdf', 'text/plain', 'text/markdown'].includes(mimeType);

      if (supportedImage) {
        promptText = "VocÃª Ã© um assistente mÃ©dico. Descreva esta imagem de forma objetiva, focando em detalhes que possam ser clinicamente relevantes. Se for um exame, descreva os achados. Se for um documento, extraia o texto e resuma-o.";
      } else if (supportedDoc) {
        promptText = "VocÃª Ã© um assistente mÃ©dico. Resuma o conteÃºdo deste documento, extraindo as informaÃ§Ãµes mais importantes como diagnÃ³sticos, tratamentos, resultados de exames e histÃ³rico do paciente.";
      } else {
        log('VertexAI', `Tipo de arquivo nÃ£o suportado para resumo: ${mimeType}`);
        return {
          fileName: file.originalname,
          summary: [`Resumo nÃ£o gerado para '${file.originalname}' - tipo de arquivo nÃ£o suportado`],
        };
      }

      promptParts.push({ text: promptText });
      promptParts.push({
        inlineData: {
          mimeType: mimeType,
          data: fileBuffer.toString('base64'),
        },
      });

      const request = {
        contents: [{ role: 'user', parts: promptParts }],
        generationConfig: {
          maxOutputTokens: 2048,
          temperature: 0.3,
        },
      };

      const result = await generativeModel.generateContent(request);
      const summaryText = result.response?.candidates?.[0]?.content?.parts?.[0]?.text;

      if (summaryText) {
        return {
          fileName: file.originalname,
          summary: summaryText.trim(),
        };
      }

      return {
        fileName: file.originalname,
        summary: [`NÃ£o foi possÃ­vel gerar um resumo para '${file.originalname}'`],
      };
    });

    const summaries = await Promise.all(summaryPromises);

    log('API', `Resumos gerados com sucesso para ${endpointName}`);
    res.status(200).json({ summaries });
  } catch (error) {
    log('API-ERROR', `Erro em ${endpointName}:`, error);
    res.status(500).json({
      error: "Ocorreu um erro no servidor ao processar os documentos.",
      details: error.message,
      stack: process.env.NODE_ENV === 'development' ? error.stack : undefined,
    });
  }
});

app.post("/api/chat", async (req, res) => {
  try {
    const {comando, history } = req.body;
    if (!history || !Array.isArray(history) || history.length === 0) {
      return res.status(400).json({ error: 'O campo "history" Ã© obrigatÃ³rio.' });
    }

    const systemPrompt = `
      VocÃª Ã© uma IA mÃ©dica, assistente de consultas. Sua saÃ­da deve ser EXCLUSIVAMENTE um JSON vÃ¡lido.
      Nunca adicione explicaÃ§Ãµes, comentÃ¡rios ou texto fora do JSON.
      VocÃª deve analisar o histÃ³rico da conversa e o Ãºltimo comando do usuÃ¡rio para determinar a resposta.

      Se o Ãºltimo comando for uma transcriÃ§Ã£o de Ã¡udio:
      - Gere um resumo clÃ­nico curto da transcriÃ§Ã£o.
      - Crie um tÃ­tulo conciso (atÃ© 10 palavras) para a consulta.
      - A saÃ­da deve ser um JSON com a estrutura:
        {
          "mensagem": "resumo clÃ­nico aqui",
          "titulo": "tÃ­tulo da consulta aqui",
          "mode": "BIGTIME"
        }

      Se o Ãºltimo comando for uma solicitaÃ§Ã£o de anamnese, ou documento, como "Gera uma Anamnese" ou "Gere Documento":
      - Analise toda a conversa anterior.
      - Gere uma anamnese/documento completa sempre em formato HTML (usando parÃ¡grafos, negrito, listas).
      - A saÃ­da deve ser um JSON com a estrutura:
        {
          "html": "anamnese/documento completa em HTML aqui",
           "titulo": "tÃ­tulo para o documento aqui",
          "mode": "HTML"
        }

      Para qualquer outro comando ou pergunta do usuÃ¡rio:
      - Responda de forma normal e Ãºtil para a conversa.
      - A saÃ­da deve ser um JSON com a estrutura:
        {
          "mensagem": "sua resposta normal aqui, pense bem antes de responder, analise o contexto geral",
          "mode": "CHATIME"
        }
    `;

    const formattedHistory = history.map(msg => ({
      role: msg.from === "user" ? "user" : "model",
      parts: [{ text: `${comando} - ${msg.text}` }]
    }));

    const previousMessages = formattedHistory.slice(0, -1);
    const lastUserMessage = formattedHistory[formattedHistory.length - 1].parts[0].text;

    const model = "gemini-2.5-pro";
    const generativeModel = vertex_ai.getGenerativeModel({
      model,
      generationConfig: { maxOutputTokens: 4048, temperature: 0.2 }
    });

    const chat = generativeModel.startChat({
      systemInstruction: { parts: [{ text: systemPrompt }] },
      history: previousMessages
    });

    const result = await chat.sendMessage(lastUserMessage);
    const responseText = result.response.candidates[0].content.parts[0].text;

    try {
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (jsonMatch && jsonMatch[0]) {
        const responseObject = JSON.parse(jsonMatch[0]);
        res.json(responseObject);
      } else {
        console.warn("A resposta da IA nÃ£o continha um JSON vÃ¡lido.");
        res.status(500).json({ mensagem: "Erro: formato de resposta da IA invÃ¡lido." });
      }
    } catch (error) {
      console.error("Erro ao fazer parse do JSON da IA:", error);
      res.status(500).json({ mensagem: "Erro interno no servidor." });
    }

  } catch (error) {
    console.error(error);
    res.status(500).json({ error: "Erro ao processar a requisiÃ§Ã£o de chat." });
  }
});

// --- Iniciar Servidor ---
const PORT = process.env.PORT || 8080;
server.listen(PORT, () => {
  log('Server', `ğŸš€ Servidor rodando na porta ${PORT}`);
  log('Server', `ğŸ”Œ WebSockets habilitados para Cloud Run`);
  log('Server', `ğŸŒ Health check disponÃ­vel em /health`);
});