// index.js
import 'dotenv/config';

import express from 'express';
import http from 'http';
import { Server } from 'socket.io';
import speech from '@google-cloud/speech';
import multer from 'multer';
import cors from 'cors';
import { Storage } from '@google-cloud/storage';
import { v4 as uuidv4 } from 'uuid';

// Vertex AI
import { VertexAI } from '@google-cloud/vertexai';

// --- FunÃ§Ãµes de Log Padronizadas ---
const log = (prefix, message, ...args) =>
Â  console.log(`[${new Date().toISOString()}] [${prefix}]`, message, ...args);

// --- ConfiguraÃ§Ãµes iniciais ---
const app = express();
const server = http.createServer(app);

// Socket.io configurado para Cloud Run
const io = new Server(server, { 
Â  cors: { 
Â  Â  origin: "*",
Â  Â  methods: ["GET", "POST"]
Â  },
Â  transports: ['websocket', 'polling'],
Â  pingTimeout: 60000,
Â  pingInterval: 25000
});

const upload = multer();

// CORS configurado para Cloud Run
app.use(cors({
Â  origin: true,
Â  credentials: true,
Â  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS']
}));

app.use(express.json());

// Health check endpoint para Cloud Run
app.get('/health', (req, res) => {
Â  res.status(200).json({ 
Â  Â  status: 'ok', 
Â  Â  socket: 'enabled',
Â  Â  timestamp: new Date().toISOString()
Â  });
});

// WebSocket test endpoint
app.get('/ws-test', (req, res) => {
Â  res.json({ 
Â  Â  status: 'ok', 
Â  Â  socketEnabled: true,
Â  Â  timestamp: new Date().toISOString()
Â  });
});

// Handle preflight
app.options('*', cors());

// --- Middleware de Logging de RequisiÃ§Ãµes ---
app.use((req, res, next) => {
Â  log('HTTP', `RequisiÃ§Ã£o recebida: ${req.method} ${req.url}`);
Â  next();
});

// --- Clientes ---
const speechClient = new speech.SpeechClient();
const storage = new Storage();
const vertex_ai = new VertexAI({
Â  project: process.env.GCLOUD_PROJECT,
Â  location: process.env.GCLOUD_LOCATION,
});

const model = 'gemini-2.0-flash-001';
const generativeModel = vertex_ai.getGenerativeModel({
Â  model,
Â  generationConfig: {
Â  Â  maxOutputTokens: 8192,
Â  Â  temperature: 0.2,
Â  },
});

/**
Â * FunÃ§Ã£o auxiliar para chamar o Vertex AI e centralizar o logging.
Â */
const callVertexAI = async (endpointName, prompt, generationConfig = {}) => {
Â  log('VertexAI', `Iniciando chamada para o endpoint: ${endpointName}`);
Â  log(
Â  Â  'VertexAI',
Â  Â  `Prompt enviado:\n---INÃCIO DO PROMPT---\n${prompt}\n---FIM DO PROMPT---`
Â  );

Â  const request = {
Â  Â  contents: [{ role: 'user', parts: [{ text: prompt }] }],
Â  Â  generationConfig: { ...generativeModel.generationConfig, ...generationConfig },
Â  };

Â  const result = await generativeModel.generateContent(request);
Â  const generatedText = result.response.candidates[0].content.parts[0].text;

Â  log('VertexAI', `Resposta recebida do endpoint ${endpointName}`);
Â  return generatedText.trim();
};

// ===================================
// --- WebSocket STT com AutomaÃ§Ã£o ---
// ===================================
io.on('connection', (socket) => {
Â  log('WebSocket', `Cliente conectado: ${socket.id} from ${socket.handshake.address}`);

Â  let recognizeStream = null;
Â  let recognitionConfig = null;
Â  let silenceTimer = null;
Â  let streamRestartTimer = null;
Â  const silenceTimeoutDuration = 10000; // 10 segundos
Â  const maxStreamDuration = 290 * 1000; // ~4.8 minutos

Â  const stopRecognizeStream = () => {
Â  Â  if (recognizeStream) {
Â  Â  Â  recognizeStream.end();
Â  Â  Â  recognizeStream = null;
Â  Â  Â  log('WebSocket', `Stream de reconhecimento encerrado para: ${socket.id}`);
Â  Â  }
Â  Â  clearTimeout(streamRestartTimer);
Â  Â  clearTimeout(silenceTimer);
Â  };

Â  const startRecognizeStream = () => {
Â  Â  if (recognizeStream || !recognitionConfig) {
Â  Â  Â  log(
Â  Â  Â  Â  'WebSocket',
Â  Â  Â  Â  `Tentativa de iniciar stream falhou (jÃ¡ iniciado ou sem config) para: ${socket.id}`
Â  Â  Â  );
Â  Â  Â  return;
Â  Â  }
Â  Â  log(
Â  Â  Â  'WebSocket',
Â  Â  Â  `Iniciando/Reiniciando stream de reconhecimento para: ${socket.id}`
Â  Â  );

Â  Â  const request = { config: recognitionConfig, interimResults: true };

Â  Â  recognizeStream = speechClient
Â  Â  Â  .streamingRecognize(request)
Â  Â  Â  .on('error', (err) => {
Â  Â  Â  Â  log('SpeechAPI-ERROR', `Erro no streaming para ${socket.id}:`, err.message);
Â  Â  Â  Â  socket.emit('error', 'Erro no reconhecimento de fala.');
Â  Â  Â  Â  stopRecognizeStream();
Â  Â  Â  })
Â  Â  Â  .on('data', (data) => {
Â  Â  Â  Â  const result = data.results[0];
Â  Â  Â  Â  if (result && result.alternatives[0]) {
Â  Â  Â  Â  Â  const transcriptData = {
Â  Â  Â  Â  Â  Â  text: result.alternatives[0].transcript,
Â  Â  Â  Â  Â  Â  isFinal: result.isFinal,
Â  Â  Â  Â  Â  Â  timestamp: new Date().toLocaleTimeString('pt-BR', {
Â  Â  Â  Â  Â  Â  Â  hour: '2-digit',
Â  Â  Â  Â  Â  Â  Â  minute: '2-digit',
Â  Â  Â  Â  Â  Â  }),
Â  Â  Â  Â  Â  Â  speakerTag:
Â  Â  Â  Â  Â  Â  Â  result.alternatives[0].words?.[
Â  Â  Â  Â  Â  Â  Â  Â  result.alternatives[0].words.length - 1
Â  Â  Â  Â  Â  Â  Â  ]?.speakerTag,
Â  Â  Â  Â  Â  };
Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  log('WebSocket', `Enviando transcript: ${transcriptData.text.substring(0, 50)}...`);
Â  Â  Â  Â  Â  socket.emit('transcript-data', transcriptData);
Â  Â  Â  Â  }
Â  Â  Â  });

Â  Â  // reinÃ­cio automÃ¡tico do stream depois de 4.8 minutos
Â  Â  streamRestartTimer = setTimeout(() => {
Â  Â  Â  log(
Â  Â  Â  Â  'WebSocket',
Â  Â  Â  Â  `Stream atingiu a duraÃ§Ã£o mÃ¡xima de ${
Â  Â  Â  Â  Â  maxStreamDuration / 1000
Â  Â  Â  Â  }s. Reiniciando para ${socket.id}...`
Â  Â  Â  );
Â  Â  Â  stopRecognizeStream();
Â  Â  Â  startRecognizeStream();
Â  Â  }, maxStreamDuration);
Â  };

Â  const resetSilenceTimer = () => {
Â  Â  clearTimeout(silenceTimer);
Â  Â  silenceTimer = setTimeout(() => {
Â  Â  Â  log(
Â  Â  Â  Â  'WebSocket',
Â  Â  Â  Â  `SilÃªncio detectado para ${socket.id}. Reiniciando recognizeStream no servidor.`
Â  Â  Â  );

Â  Â  Â  // ğŸŸ¢ reinicia internamente sem pedir nada ao cliente
Â  Â  Â  stopRecognizeStream();
Â  Â  Â  startRecognizeStream();
Â  Â  }, silenceTimeoutDuration);
Â  };

Â  socket.on('start-recording', (config) => {
Â  Â  log('WebSocket', `Evento 'start-recording' recebido de ${socket.id}`, config);
Â  Â  recognitionConfig = {
Â  Â  Â  encoding: 'WEBM_OPUS',
Â  Â  Â  sampleRateHertz: config.sampleRateHertz || 48000,
Â  Â  Â  languageCode: config.lang || 'pt-BR',
Â  Â  Â  alternativeLanguageCodes: ['en-US'], 
Â  Â  Â  enableAutomaticPunctuation: true,
Â  Â  Â  diarizationConfig: {
Â  Â  Â  Â  enableSpeakerDiarization: true,
Â  Â  Â  Â  minSpeakerCount: 2,
Â  Â  Â  Â  maxSpeakerCount: 6,
Â  Â  Â  },
Â  Â  Â  model: 'telephony',
Â  Â  Â  useEnhanced: true,
Â  Â  };
Â  Â  stopRecognizeStream();
Â  Â  startRecognizeStream();
Â  Â  resetSilenceTimer();
Â  });

Â  socket.on('audio-data', (data) => {
Â  Â  if (recognizeStream && data) {
Â  Â  Â  recognizeStream.write(data);
Â  Â  Â  resetSilenceTimer();
Â  Â  } else if (!recognizeStream) {
Â  Â  Â  log(
Â  Â  Â  Â  'WebSocket',
Â  Â  Â  Â  `Recebido 'audio-data' de ${socket.id}, mas o stream nÃ£o estÃ¡ pronto. Ignorando chunk.`
Â  Â  Â  );
Â  Â  }
Â  });

Â  socket.on('force-flush-partial', (partial) => {
Â  Â  log('WebSocket', `Evento 'force-flush-partial' recebido de ${socket.id}`);
Â  Â  socket.emit('transcript-data', { ...partial, isFinal: true });
Â  });

Â  socket.on('stop-recording', () => {
Â  Â  log('WebSocket', `Evento 'stop-recording' recebido de ${socket.id}`);
Â  Â  stopRecognizeStream();
Â  });

Â  socket.on('disconnect', (reason) => {
Â  Â  log('WebSocket', `Cliente desconectado: ${socket.id} - Reason: ${reason}`);
Â  Â  stopRecognizeStream();
Â  });
});

// ======================
// --- Batch STT ---
// ======================
app.post('/batch-transcribe', upload.single('file'), async (req, res) => {
Â  const endpointName = '/batch-transcribe';
Â  log('API', `Iniciando ${endpointName}`);
Â  try {
Â  Â  if (!req.file) {
Â  Â  Â  log('API-ERROR', `${endpointName} - Nenhum arquivo enviado.`);
Â  Â  Â  return res.status(400).json({ error: 'Nenhum arquivo enviado.' });
Â  Â  }

Â  Â  const audioBuffer = req.file.buffer;
Â  Â  const bucketName = process.env.GCLOUD_BUCKET_NAME;
Â  Â  const recordingId = uuidv4();
Â  Â  const filename = `audio-${recordingId}.opus`;
Â  Â  const gcsUri = `gs://${bucketName}/${filename}`;

Â  Â  log('GCS', `Fazendo upload de ${filename} para o bucket ${bucketName}`);
Â  Â  await storage.bucket(bucketName).file(filename).save(audioBuffer, {
Â  Â  Â  metadata: { contentType: req.file.mimetype },
Â  Â  });
Â  Â  log('GCS', `Upload concluÃ­do: ${gcsUri}`);

Â  Â  log('SpeechAPI', `Iniciando 'longRunningRecognize' para ${gcsUri}`);
Â  Â  const [operation] = await speechClient.longRunningRecognize({
Â  Â  Â  audio: { uri: gcsUri },
Â  Â  Â  config: {
Â  Â  Â  Â  encoding: 'WEBM_OPUS',
Â  Â  Â  Â  sampleRateHertz: 48000,
Â  Â  Â  Â  languageCode: 'pt-BR',
Â  Â  Â  Â  alternativeLanguageCodes: ['en-US'],
Â  Â  Â  Â  enableAutomaticPunctuation: true,
Â  Â  Â  Â  diarizationConfig: {
Â  Â  Â  Â  Â  enableSpeakerDiarization: true,
Â  Â  Â  Â  Â  minSpeakerCount: 2,
Â  Â  Â  Â  Â  maxSpeakerCount: 6,
Â  Â  Â  Â  },
Â  Â  Â  },
Â  Â  });

Â  Â  const [response] = await operation.promise();
Â  Â  log('SpeechAPI', `'longRunningRecognize' concluÃ­do para ${gcsUri}`);

Â  Â  const structuredTranscript = [];
Â  Â  let currentSegment = null;
Â  Â  let lastSpeakerTag = null;

Â  Â  response.results.forEach(result => {
Â  Â  Â  const alternative = result.alternatives[0];
Â  Â  Â  const transcriptText = alternative?.transcript?.trim();
Â  Â  Â  const words = alternative?.words;

Â  Â  Â  if (!transcriptText || !words || words.length === 0) return;

Â  Â  Â  const firstWord = words[0];
Â  Â  Â  const speakerTag = firstWord.speakerTag;

Â  Â  Â  if (speakerTag !== lastSpeakerTag) {
Â  Â  Â  Â  currentSegment = {
Â  Â  Â  Â  Â  text: transcriptText,
Â  Â  Â  Â  Â  isFinal: true,
Â  Â  Â  Â  Â  speakerTag: speakerTag,
Â  Â  Â  Â  Â  timestamp: firstWord.startTime.seconds ? new Date(firstWord.startTime.seconds * 1000).toLocaleTimeString('pt-BR', { hour: '2-digit', minute: '2-digit' }) : '00:00',
Â  Â  Â  Â  };
Â  Â  Â  Â  structuredTranscript.push(currentSegment);
Â  Â  Â  Â  lastSpeakerTag = speakerTag;
Â  Â  Â  } else if (currentSegment) {
Â  Â  Â  Â  currentSegment.text += ' ' + transcriptText;
Â  Â  Â  }
Â  Â  });

Â  Â  log('API', `TranscriÃ§Ã£o em lote estruturada com ${structuredTranscript.length} segmentos.`);
Â  Â  res.json({ recordingId, audioUri: gcsUri, batchTranscript: structuredTranscript });

Â  } catch (err) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, err);
Â  Â  res.status(500).json({ error: 'Falha na transcriÃ§Ã£o em lote.' });
Â  }
});

// ==========================
// --- Endpoints Vertex AI ---
// ==========================

// --- GERAÃ‡ÃƒO DE TÃTULO ---
app.post('/api/generate-title', async (req, res) => {
Â  const endpointName = '/api/generate-title';
Â  try {
Â  Â  const { context } = req.body;
Â  Â  if (!context || typeof context !== 'string' || context.trim() === '') {
Â  Â  Â  return res.status(400).json({ error: 'O campo "context" Ã© obrigatÃ³rio.' });
Â  Â  }

Â  Â  const prompt = `
Â  Â  Â  VocÃª Ã© um assistente especializado em criar tÃ­tulos curtos e objetivos para consultas mÃ©dicas.
Â  Â  Â  Baseado no contexto abaixo, gere um tÃ­tulo conciso (mÃ¡x. 10 palavras) que resuma o motivo principal da consulta.
Â  Â  Â  O tÃ­tulo deve ser claro, direto e fÃ¡cil de entender. NÃ£o use markdown (como **, #) na resposta.

Â  Â  Â  Contexto: "${context}"

Â  Â  Â  TÃ­tulo Gerado:
Â  Â  `;

Â  Â  const generatedTitle = await callVertexAI(endpointName, prompt);
Â  Â  res.status(200).json({ title: generatedTitle });

Â  } catch (error) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, error);
Â  Â  res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar o tÃ­tulo.' });
Â  }
});

// --- MELHORAR ANAMNESE ---
app.post('/api/melhorar-anamnese', async (req, res) => {
Â  Â  const endpointName = '/api/melhorar-anamnese';
Â  Â  try {
Â  Â  Â  Â  const { anamnese, prompt } = req.body;

Â  Â  Â  Â  if (!anamnese || typeof anamnese !== 'string' || anamnese.trim() === '') {
Â  Â  Â  Â  Â  Â  return res.status(400).json({ error: 'O campo "anamnese" Ã© obrigatÃ³rio.' });
Â  Â  Â  Â  }
Â  Â  Â  Â  if (!prompt || typeof prompt !== 'string' || prompt.trim() === '') {
Â  Â  Â  Â  Â  Â  return res.status(400).json({ error: 'O campo "prompt" (instruÃ§Ã£o) Ã© obrigatÃ³rio.' });
Â  Â  Â  Â  }

Â  Â  Â  Â  const structuredPrompt = `
Â  Â  Â  Â  Â  Â  ### Persona
Â  Â  Â  Â  Â  Â  Aja como um assistente mÃ©dico redator, especialista em criar documentos clÃ­nicos claros, objetivos e bem estruturados.

Â  Â  Â  Â  Â  Â  ### Contexto
Â  Â  Â  Â  Â  Â  O texto de uma anamnese mÃ©dica precisa ser refinado com base em uma instruÃ§Ã£o especÃ­fica do mÃ©dico.

Â  Â  Â  Â  Â  Â  ### Tarefa
Â  Â  Â  Â  Â  Â  Reescreva o "Texto Original da Anamnese" abaixo, seguindo estritamente a "InstruÃ§Ã£o do MÃ©dico".

Â  Â  Â  Â  Â  Â  ### Requisitos
Â  Â  Â  Â  Â  Â  - O formato da resposta DEVE ser um Ãºnico bloco de texto usando tags HTML simples (<p>, <strong>, <ul>, <li>).
Â  Â  Â  Â  Â  Â  - O tom deve ser formal, tÃ©cnico e objetivo.
Â  Â  Â  Â  Â  Â  - Mantenha TODAS as informaÃ§Ãµes clÃ­nicas originais. NÃƒO omita e NÃƒO invente dados.
Â  Â  Â  Â  Â  Â  - Corrija erros gramaticais.

Â  Â  Â  Â  Â  Â  ### Dados de Entrada
Â  Â  Â  Â  Â  Â  **InstruÃ§Ã£o do MÃ©dico:**
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  ${prompt}
Â  Â  Â  Â  Â  Â  """

Â  Â  Â  Â  Â  Â  **Texto Original da Anamnese:**
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  ${anamnese}
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  `;

Â  Â  Â  Â  const enhancedAnamnese = await callVertexAI(endpointName, structuredPrompt);
Â  Â  Â  Â  res.status(200).json({ enhancedAnamnese });

Â  Â  } catch (error) {
Â  Â  Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, error);
Â  Â  Â  Â  res.status(500).json({ error: 'Ocorreu um erro no servidor ao processar a solicitaÃ§Ã£o.' });
Â  Â  }
});

// --- ROTA DE TRANSCRIÃ‡ÃƒO IA ---
app.post('/api/generate-ia-transcription', async (req, res) => {
Â  try {
Â  Â  const { transcription } = req.body;

Â  Â  if (!transcription || typeof transcription !== 'string' || transcription.trim() === '') {
Â  Â  Â  return res.status(400).json({
Â  Â  Â  Â  error: 'O campo "transcription" com o array de transcriÃ§Ãµes Ã© obrigatÃ³rio.'
Â  Â  Â  });
Â  Â  }

Â  Â  let allTranscripts;
Â  Â  try {
Â  Â  Â  allTranscripts = JSON.parse(transcription);
Â  Â  Â  if (!Array.isArray(allTranscripts)) throw new Error("Formato invÃ¡lido.");
Â  Â  } catch (e) {
Â  Â  Â  return res.status(400).json({ error: 'O campo "transcription" deve ser um JSON array vÃ¡lido.' });
Â  Â  }

Â  Â  const context = allTranscripts.slice(0, -1);
Â  Â  const newTranscriptToProcess = allTranscripts.slice(-1);

Â  Â  const contextString = context.length > 0
Â  Â  Â  ? `
Contexto da Conversa (diÃ¡logo anterior):
${JSON.stringify(context.map(t => ({ speaker: t.speaker, text: t.text })), null, 2)}
`
Â  Â  Â  : "Esta Ã© a primeira fala da conversa.";

Â  Â  const prompt = `
VocÃª Ã© um assistente de IA especialista em processar transcriÃ§Ãµes de consultas mÃ©dicas.

âš™ï¸ InstruÃ§Ã£o de Idioma:
- Detecte automaticamente o idioma da "Nova TranscriÃ§Ã£o".
- Se o idioma predominante for **inglÃªs**, todas as respostas e o conteÃºdo do JSON devem ser **em inglÃªs** (inclusive nomes de campos e tÃ³picos da timeline).
- Caso contrÃ¡rio, use **portuguÃªs** como idioma padrÃ£o.
- NÃƒO traduza o conteÃºdo da transcriÃ§Ã£o; apenas mantenha o idioma original da conversa para todo o processamento.

Suas tarefas sÃ£o:
1. Analisar a "Nova TranscriÃ§Ã£o", usando o "Contexto da Conversa" para manter a consistÃªncia na identificaÃ§Ã£o de "MÃ©dico" e "Paciente".
2. Criar e atualizar uma "timeline" (uma lista cronolÃ³gica) dos principais assuntos discutidos em TODA a conversa (contexto + nova transcriÃ§Ã£o).
3. Formatar a saÃ­da como um objeto JSON contendo a transcriÃ§Ã£o processada e a timeline de assuntos.

InstruÃ§Ãµes Detalhadas:
1. Processamento da TranscriÃ§Ã£o:
Â  Â - Corrija erros gramaticais na "Nova TranscriÃ§Ã£o".
Â  Â - Mantenha a consistÃªncia dos papÃ©is ("MÃ©dico", "Paciente").
Â  Â - A transcriÃ§Ã£o processada deve ser um array de objetos, cada objeto representando uma fala Ãºnica, com os campos obrigatÃ³rios:
Â  Â  Â - "speakerTag"
Â  Â  Â - "speaker"
Â  Â  Â - "text"
Â  Â  Â - "timestamp"
Â  Â  Â - "isFinal"
Â  Â - ATENÃ‡ÃƒO: O campo "isFinal" DEVE SEMPRE ser \`true\` em todos os itens do processedTranscript.
Â  Â - Caso o texto esteja vazio ("" ou apenas espaÃ§os), nÃ£o deve ser incluÃ­do no processedTranscript.

2. GeraÃ§Ã£o da Timeline de Assuntos:
Â  Â - Analise o diÃ¡logo completo (contexto + nova transcriÃ§Ã£o).
Â  Â - Identifique os tÃ³picos principais (ex: "ApresentaÃ§Ã£o de sintomas", "HistÃ³rico do paciente", "DiscussÃ£o sobre dor de cabeÃ§a", "DiagnÃ³stico inicial", "PrescriÃ§Ã£o de medicaÃ§Ã£o").
Â  Â - A timeline deve ser um array de strings.
Â  Â - A cada nova chamada, vocÃª deve retornar a timeline completa e atualizada, adicionando novos tÃ³picos conforme eles surgem.

3. Formato de SaÃ­da OBRIGATÃ“RIO:
Â  Â - Sua resposta DEVE SER um Ãºnico objeto JSON, sem nenhum texto ou markdown em volta.
Â  Â - O objeto deve ter duas chaves: "processedTranscript" (um array de objetos, cada um representando uma fala) e "timeline" (um array de strings).

---
${contextString}
---
Nova TranscriÃ§Ã£o para processar:
${JSON.stringify(newTranscriptToProcess, null, 2)}
`;

Â  Â  const request = {
Â  Â  Â  contents: [{ role: 'user', parts: [{ text: prompt }] }],
Â  Â  Â  generationConfig: {
Â  Â  Â  Â  maxOutputTokens: 4048,
Â  Â  Â  Â  temperature: 0.3,
Â  Â  Â  },
Â  Â  };

Â  Â  const result = await generativeModel.generateContent(request);
Â  Â  let generatedText = result.response?.candidates?.[0]?.content?.parts?.[0]?.text;

Â  Â  if (!generatedText) {
Â  Â  Â  throw new Error('Resposta vazia do modelo generativo');
Â  Â  }
Â  Â  
Â  Â  generatedText = generatedText.trim().replace(/^```json\s*|```$/g, "").trim();
Â  Â  
Â  Â  let parsedJson;
Â  Â  try {
Â  Â  Â  parsedJson = JSON.parse(generatedText);
Â  Â  Â  if (!parsedJson.processedTranscript || !Array.isArray(parsedJson.timeline)) {
Â  Â  Â  Â  throw new Error("A resposta da IA nÃ£o contÃ©m os campos 'processedTranscript' e 'timeline'.");
Â  Â  Â  }
Â  Â  } catch (err) {
Â  Â  Â  log('API-ERROR', "Erro ao fazer parse do JSON da IA:", err, generatedText); // Corrigido
Â  Â  Â  return res.status(500).json({ error: "Falha ao processar resposta da IA. Formato JSON invÃ¡lido." });
Â  Â  }

Â  Â  res.status(200).json({ data: parsedJson });

Â  } catch (error) {
Â  Â  log('API-ERROR', 'Erro ao gerar transcriÃ§Ã£o via Vertex AI:', error); // Corrigido
Â  Â  res.status(500).json({ error: 'Ocorreu um erro no servidor ao processar a transcriÃ§Ã£o.' });
Â  }
});

// --- GERAÃ‡ÃƒO DE RESUMO ---
app.post('/api/generate-summary', async (req, res) => {
Â  const endpointName = '/api/generate-summary';
Â  try {
Â  Â  const { transcription } = req.body;
Â  Â  if (!transcription || !Array.isArray(transcription) || transcription.length === 0) {
Â  Â  Â  return res.status(400).json({ error: 'O campo "transcription" Ã© obrigatÃ³rio e deve ser um array.' });
Â  Â  }

Â  Â  const formattedTranscription = transcription.map(item => `${item.speakerTag || 'Pessoa'}: ${item.text}`).join('\n');

Â  Â  const prompt = `
Â  Â  Â  VocÃª Ã© um assistente de IA focado em transcriÃ§Ãµes mÃ©dicas. Sua tarefa Ã© gerar dois resultados claros, sem usar markdown ou introduÃ§Ãµes.

Â  Â  Â  1. **Resumo da TranscriÃ§Ã£o**: Crie um resumo objetivo da consulta.
Â  Â  Â  2. **AvaliaÃ§Ã£o da TranscriÃ§Ã£o**:
Â  Â  Â  Â  Â - Comente se a transcriÃ§Ã£o contÃ©m informaÃ§Ãµes suficientes e coerentes.
Â  Â  Â  Â  Â - Aponte lacunas ou inconsistÃªncias.
Â  Â  Â  Â  Â - Avalie se faz sentido, no contexto mÃ©dico, usar IA para gerar resumos desta transcriÃ§Ã£o.

Â  Â  Â  A transcriÃ§Ã£o Ã© a seguinte:
Â  Â  Â  "${formattedTranscription}"
Â  Â  `;

Â  Â  const generatedSummary = await callVertexAI(endpointName, prompt);
Â  Â  res.status(200).json({ summary: generatedSummary });

Â  } catch (error) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, error);
Â  Â  res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar o resumo.' });
Â  }
});

// --- GERAÃ‡ÃƒO DE ANAMNESE ---
app.post('/api/generate-anamnese', async (req, res) => {
Â  const endpointName = '/api/generate-anamnese';
Â  try {
Â  Â  const { transcription, prompt, documentoSelecionado } = req.body;

Â  Â  if (!transcription || !Array.isArray(transcription) || transcription.length === 0 || !documentoSelecionado) {
Â  Â  Â  return res.status(400).json({ error: 'Campos obrigatÃ³rios: transcription, documentoSelecionado.' });
Â  Â  }

Â  Â  const formattedTranscript = transcription.map(line => `${line.speakerTag}: ${line.text}`).join('\n');

Â  Â  const fullPrompt = `
Â  Â  Â  VocÃª Ã© um assistente mÃ©dico virtual que sumariza conversas clÃ­nicas em anamneses estruturadas.
Â  Â  Â  Sua tarefa Ã© analisar a transcriÃ§Ã£o de uma consulta e gerar uma anamnese completa.

Â  Â  Â  InstruÃ§Ãµes Adicionais:
Â  Â  Â  ${prompt ? ` - Contexto do Paciente: "${prompt}"` : ''}
Â  Â  Â  
Â  Â  Â  O Documento deve conter as seguintes seÃ§Ãµes obrigatÃ³rias:
Â  Â  Â  ${documentoSelecionado}

Â  Â  Â  Formate o resultado em um Ãºnico bloco de texto usando HTML (parÃ¡grafos, negrito, listas).

Â  Â  Â  TranscriÃ§Ã£o da consulta:
Â  Â  Â  "${formattedTranscript}"

Â  Â  Â  Anamnese Gerada (formato HTML):
Â  Â  `;

Â  Â  const generatedAnamnese = await callVertexAI(endpointName, fullPrompt);
Â  Â  res.status(200).json({ anamnese: generatedAnamnese });

Â  } catch (error) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, error);
Â  Â  res.status(500).json({ error: 'Ocorreu um erro no servidor ao gerar a anamnese.' });
Â  }
});

// --- OBTENÃ‡ÃƒO DE URL DE ÃUDIO ---
app.get('/audio-url/:recordingId', async (req, res) => {
Â  const endpointName = '/audio-url/:recordingId';
Â  try {
Â  Â  const bucketName = process.env.GCLOUD_BUCKET_NAME;
Â  Â  const { recordingId } = req.params;
Â  Â  
Â  Â  // Corrigido: o nome do arquivo deve incluir o prefixo 'audio-' e a extensÃ£o '.opus'
Â  Â  const filename = `audio-${recordingId}.opus`;

Â  Â  log('GCS', `Buscando URL assinada para ${filename}`);
Â  Â  const file = storage.bucket(bucketName).file(filename);

Â  Â  const [exists] = await file.exists();
Â  Â  if (!exists) {
Â  Â  Â  log('GCS-ERROR', `Arquivo nÃ£o encontrado: ${filename}`);
Â  Â  Â  return res.status(404).json({ error: 'Arquivo de Ã¡udio nÃ£o encontrado.' });
Â  Â  }

Â  Â  const [signedUrl] = await file.getSignedUrl({
Â  Â  Â  action: 'read',
Â  Â  Â  expires: Date.now() + 15 * 60 * 1000, // 15 minutos
Â  Â  });

Â  Â  log('GCS', `URL assinada gerada com sucesso.`);
Â  Â  res.json({ audioUrl: signedUrl });

Â  } catch (err) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, err);
Â  Â  res.status(500).json({ error: 'Falha ao gerar URL de Ã¡udio.' });
Â  }
});

app.delete('/audio/:recordingId', async (req, res) => {
Â  const endpointName = '/audio/:recordingId';
Â  try {
Â  Â  const { recordingId } = req.params;

Â  Â  if (!recordingId) {
Â  Â  Â  return res.status(400).json({ error: 'ID de gravaÃ§Ã£o invÃ¡lido.' });
Â  Â  }

Â  Â  const bucketName = process.env.GCLOUD_BUCKET_NAME;
Â  Â  if (!bucketName) {
Â  Â  Â  Â  log('API-ERROR', 'A variÃ¡vel de ambiente GCLOUD_BUCKET_NAME nÃ£o estÃ¡ definida.');
Â  Â  Â  Â  return res.status(500).json({ error: 'ConfiguraÃ§Ã£o do servidor incompleta.'});
Â  Â  }

    // ğŸ’¡ CORREÃ‡ÃƒO LÃ“GICA: Montar o nome completo do arquivo
    const filename = `audio-${recordingId}.opus`;
Â  Â  const file = storage.bucket(bucketName).file(filename);

Â  Â  const [exists] = await file.exists();
Â  Â  if (!exists) {
Â  Â  Â  log('GCS-ERROR', `Arquivo nÃ£o encontrado no bucket '${bucketName}': ${filename}`);
Â  Â  Â  return res.status(404).json({ error: 'Arquivo de Ã¡udio nÃ£o encontrado.' });
Â  Â  }

Â  Â  await file.delete();
Â  Â  log('GCS', `Arquivo ${filename} removido do bucket ${bucketName} com sucesso.`);
Â  Â  res.status(200).json({ message: 'Arquivo de Ã¡udio removido com sucesso.' });

Â  } catch (err) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, err);
Â  Â  res.status(500).json({ error: 'Falha interna ao remover arquivo de Ã¡udio.' });
Â  }
});

app.post("/api/upload-documento", upload.single("file"), async (req, res) => {
Â  try {
Â  Â  if (!req.file) {
Â  Â  Â  return res.status(400).json({ error: "Nenhum arquivo enviado." });
Â  Â  }

Â  Â  const bucketName = process.env.GCLOUD_BUCKET_DOC;
Â  Â  const bucket = storage.bucket(bucketName);
Â  Â  const gcsFileName = `${Date.now()}_${req.file.originalname}`;
Â  Â  const file = bucket.file(gcsFileName);

Â  Â  await file.save(req.file.buffer, {
Â  Â  Â  contentType: req.file.mimetype,
Â  Â  Â  resumable: false,
Â  Â  });

Â  Â  const publicUrl = `https://storage.googleapis.com/${bucketName}/${gcsFileName}`;
Â  Â  res.json({ url: publicUrl });
Â  } catch (error) {
Â  Â  log('API-ERROR', "Erro ao enviar arquivo:", error); // Corrigido
Â  Â  res.status(500).json({ error: "Erro ao enviar arquivo." });
Â  }
});

app.post("/api/process-and-summarize-documents", upload.array("documentos", 5), async (req, res) => {
Â  const endpointName = "/api/process-and-summarize-documents";
Â  log('API', `Iniciando ${endpointName}`);

Â  if (!req.files || req.files.length === 0) {
Â  Â  return res.status(400).json({ error: "Nenhum documento enviado." });
Â  }

Â  try {
Â  Â  const summaryPromises = req.files.map(async (file) => {
Â  Â  Â  log('VertexAI', `Processando arquivo: ${file.originalname} (${file.mimetype})`);

Â  Â  Â  const fileBuffer = file.buffer;
Â  Â  Â  const mimeType = file.mimetype;

Â  Â  Â  const promptParts = [];
Â  Â  Â  let promptText = "";

Â  Â  Â  const supportedImage = mimeType.startsWith('image/');
Â  Â  Â  const supportedDoc = ['application/pdf', 'text/plain', 'text/markdown'].includes(mimeType);

Â  Â  Â  if (supportedImage) {
Â  Â  Â  Â  promptText = "VocÃª Ã© um assistente mÃ©dico. Descreva esta imagem de forma objetiva, focando em detalhes que possam ser clinicamente relevantes. Se for um exame, descreva os achados. Se for um documento, extraia o texto e resuma-o.";
Â  Â  Â  } else if (supportedDoc) {
Â  Â  Â  Â  promptText = "VocÃª Ã© um assistente mÃ©dico. Resuma o conteÃºdo deste documento, extraindo as informaÃ§Ãµes mais importantes como diagnÃ³sticos, tratamentos, resultados de exames e histÃ³rico do paciente.";
Â  Â  Â  } else {
Â  Â  Â  Â  log('VertexAI', `Tipo de arquivo nÃ£o suportado para resumo: ${mimeType}`);
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  fileName: file.originalname,
Â  Â  Â  Â  Â  summary: [`Resumo nÃ£o gerado para '${file.originalname}' - tipo de arquivo nÃ£o suportado`],
Â  Â  Â  Â  };
Â  Â  Â  }

Â  Â  Â  promptParts.push({ text: promptText });
Â  Â  Â  promptParts.push({
Â  Â  Â  Â  inlineData: {
Â  Â  Â  Â  Â  mimeType: mimeType,
Â  Â  Â  Â  Â  data: fileBuffer.toString('base64'),
Â  Â  Â  Â  },
Â  Â  Â  });

Â  Â  Â  const request = {
Â  Â  Â  Â  contents: [{ role: 'user', parts: promptParts }],
Â  Â  Â  Â  generationConfig: {
Â  Â  Â  Â  Â  maxOutputTokens: 2048,
Â  Â  Â  Â  Â  temperature: 0.3,
Â  Â  Â  Â  },
Â  Â  Â  };

Â  Â  Â  const result = await generativeModel.generateContent(request);
Â  Â  Â  const summaryText = result.response?.candidates?.[0]?.content?.parts?.[0]?.text;

Â  Â  Â  if (summaryText) {
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  fileName: file.originalname,
Â  Â  Â  Â  Â  summary: summaryText.trim(),
Â  Â  Â  Â  };
Â  Â  Â  }

Â  Â  Â  return {
Â  Â  Â  Â  fileName: file.originalname,
Â  Â  Â  Â  summary: [`NÃ£o foi possÃ­vel gerar um resumo para '${file.originalname}'`],
Â  Â  Â  };
Â  Â  });

Â  Â  const summaries = await Promise.all(summaryPromises);

Â  Â  log('API', `Resumos gerados com sucesso para ${endpointName}`);
Â  Â  res.status(200).json({ summaries });
Â  } catch (error) {
Â  Â  log('API-ERROR', `Erro em ${endpointName}:`, error);
Â  Â  res.status(500).json({
Â  Â  Â  error: "Ocorreu um erro no servidor ao processar os documentos.",
Â  Â  Â  details: error.message,
Â  Â  Â  stack: process.env.NODE_ENV === 'development' ? error.stack : undefined,
Â  Â  });
Â  }
});

app.post("/api/chat", async (req, res) => {
Â  try {
Â  Â  const {comando, history } = req.body;
Â  Â  if (!history || !Array.isArray(history) || history.length === 0) {
Â  Â  Â  return res.status(400).json({ error: 'O campo "history" Ã© obrigatÃ³rio.' });
Â  Â  }

Â  Â  const systemPrompt = `
Â  Â  Â  VocÃª Ã© uma IA mÃ©dica, assistente de consultas. Sua saÃ­da deve ser EXCLUSIVAMENTE um JSON vÃ¡lido.
Â  Â  Â  Nunca adicione explicaÃ§Ãµes, comentÃ¡rios ou texto fora do JSON.
Â  Â  Â  VocÃª deve analisar o histÃ³rico da conversa e o Ãºltimo comando do usuÃ¡rio para determinar a resposta.

Â  Â  Â  Se o Ãºltimo comando for uma transcriÃ§Ã£o de Ã¡udio:
Â  Â  Â  - Gere um resumo clÃ­nico curto da transcriÃ§Ã£o.
Â  Â  Â  - Crie um tÃ­tulo conciso (atÃ© 10 palavras) para a consulta.
Â  Â  Â  - A saÃ­da deve ser um JSON com a estrutura:
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  "mensagem": "resumo clÃ­nico aqui",
Â  Â  Â  Â  Â  "titulo": "tÃ­tulo da consulta aqui",
Â  Â  Â  Â  Â  "mode": "BIGTIME"
Â  Â  Â  Â  }

Â  Â  Â  Se o Ãºltimo comando for uma solicitaÃ§Ã£o de anamnese, ou documento, como "Gera uma Anamnese" ou "Gere Documento":
Â  Â  Â  - Analise toda a conversa anterior.
Â  Â  Â  - Gere uma anamnese/documento completa sempre em formato HTML (usando parÃ¡grafos, negrito, listas).
Â  Â  Â  - A saÃ­da deve ser um JSON com a estrutura:
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  "html": "anamnese/documento completa em HTML aqui",
Â  Â  Â  Â  Â  Â "titulo": "tÃ­tulo para o documento aqui",
Â  Â  Â  Â  Â  "mode": "HTML"
Â  Â  Â  Â  }

Â  Â  Â  Para qualquer outro comando ou pergunta do usuÃ¡rio:
Â  Â  Â  - Responda de forma normal e Ãºtil para a conversa.
Â  Â  Â  - A saÃ­da deve ser um JSON com a estrutura:
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  "mensagem": "sua resposta normal aqui, pense bem antes de responder, analise o contexto geral",
Â  Â  Â  Â  Â  "mode": "CHATIME"
Â  Â  Â  Â  }
Â  Â  `;

Â  Â  const formattedHistory = history.map(msg => ({
Â  Â  Â  role: msg.from === "user" ? "user" : "model",
Â  Â  Â  parts: [{ text: `${comando} - ${msg.text}` }]
Â  Â  }));

Â  Â  const previousMessages = formattedHistory.slice(0, -1);
Â  Â  const lastUserMessage = formattedHistory[formattedHistory.length - 1].parts[0].text;

Â  Â  // Alterado para gemini-2.5-flash para melhor performance/custo em chat, 
    // ou mantido o pro se for estritamente necessÃ¡rio para raciocÃ­nio complexo. 
    // Mantenho o 'pro' por ser a sua escolha original:
Â  Â  const model = "gemini-2.5-pro"; 
Â  Â  const generativeModel = vertex_ai.getGenerativeModel({
Â  Â  Â  model,
Â  Â  Â  generationConfig: { maxOutputTokens: 4048, temperature: 0.2 }
Â  Â  });

Â  Â  const chat = generativeModel.startChat({
Â  Â  Â  systemInstruction: { parts: [{ text: systemPrompt }] },
Â  Â  Â  history: previousMessages
Â  Â  });

Â  Â  const result = await chat.sendMessage(lastUserMessage);
Â  Â  const responseText = result.response.candidates[0].content.parts[0].text;

Â  Â  try {
Â  Â  Â  // Limpeza e tentativa de parse do JSON.
Â  Â  Â  const jsonMatch = responseText.match(/\{[\s\S]*\}/);
Â  Â  Â  if (jsonMatch && jsonMatch[0]) {
Â  Â  Â  Â  const responseObject = JSON.parse(jsonMatch[0]);
Â  Â  Â  Â  res.json(responseObject);
Â  Â  Â  } else {
Â  Â  Â  Â  log('API-WARN', "A resposta da IA nÃ£o continha um JSON vÃ¡lido."); // Corrigido
Â  Â  Â  Â  res.status(500).json({ mensagem: "Erro: formato de resposta da IA invÃ¡lido." });
Â  Â  Â  }
Â  Â  } catch (error) {
Â  Â  Â  log('API-ERROR', "Erro ao fazer parse do JSON da IA:", error); // Corrigido
Â  Â  Â  res.status(500).json({ mensagem: "Erro interno no servidor." });
Â  Â  }

Â  } catch (error) {
Â  Â  log('API-ERROR', "Erro ao processar a requisiÃ§Ã£o de chat.", error); // Corrigido
Â  Â  res.status(500).json({ error: "Erro ao processar a requisiÃ§Ã£o de chat." });
Â  }
});

// --- Iniciar Servidor ---
const PORT = process.env.PORT || 8080;
server.listen(PORT, () => {
Â  log('Server', `ğŸš€ Servidor rodando na porta ${PORT}`);
Â  log('Server', `ğŸ”Œ WebSockets habilitados para Cloud Run`);
Â  log('Server', `ğŸŒ Health check disponÃ­vel em /health`);
});